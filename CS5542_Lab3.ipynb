{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinana2k/Comp-Sci-5542-Tina-Nguyen/blob/main/CS5542_Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d0c34ca",
      "metadata": {
        "id": "3d0c34ca"
      },
      "source": [
        "# CS 5542 ‚Äî Lab 3: Multimodal RAG Systems & Retrieval Evaluation  \n",
        "**Text + Images/PDFs (runs offline by default; optional LLM API hook)**\n",
        "\n",
        "This notebook is a **student-ready, simplified, and fully runnable** lab workflow for **multimodal retrieval-augmented generation (RAG)**:\n",
        "- ingest **PDF text** + **image captions/filenames**\n",
        "- retrieve evidence with a lightweight baseline (TF‚ÄëIDF)\n",
        "- build a **context block** for answering\n",
        "- evaluate retrieval quality (Precision@5, Recall@10)\n",
        "- run an **ablation study** (REQUIRED)\n",
        "\n",
        "> ‚úÖ **Important:** The code is optimized for **clarity + reproducibility for students** (minimal dependencies, no keys required).  \n",
        "> It is not the ‚Äúfastest possible‚Äù or ‚Äúbest-performing‚Äù RAG system ‚Äî but it is a correct baseline that you can extend.\n",
        "\n",
        "---\n",
        "\n",
        "## Student Tasks (what you must do)\n",
        "1. **Ingest** PDFs + images from `project_data_mm/` (or use the provided sample package).  \n",
        "2. Implement / experiment with **chunking strategies** (page-based vs fixed-size).  \n",
        "3. Compare retrieval methods (at least):  \n",
        "   - **Sparse** (TF‚ÄëIDF / BM25-style)  \n",
        "   - **Dense** (optional: embeddings)  \n",
        "   - **Hybrid** (score fusion with `alpha`)  \n",
        "   - **Hybrid + rerank** (optional: reranker / LLM rerank)  \n",
        "4. Build a **multimodal context** that includes **evidence items** (text + images).  \n",
        "5. Produce the required **results table**:\n",
        "\n",
        "`Query √ó Method √ó Precision@5 √ó Recall@10 √ó Faithfulness`\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Outputs (what graders look for)\n",
        "- Printed ingestion counts (how many PDF pages/chunks, how many images)\n",
        "- A retrieval demo showing **top‚Äëk evidence** for a query\n",
        "- Evaluation metrics per method (P@5, R@10)\n",
        "- An ablation section with a small comparison table + short explanation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "734b5101",
      "metadata": {
        "id": "734b5101"
      },
      "source": [
        "## Key Parameters You Can Tune (and what they do)\n",
        "\n",
        "These parameters control retrieval + context building. **Students should change them and report what happens.**\n",
        "\n",
        "- **`TOP_K_TEXT`**: how many text chunks to consider as candidates.  \n",
        "  - Larger ‚Üí more recall, but more noise (lower precision).\n",
        "- **`TOP_K_IMAGES`**: how many image items to consider as candidates.  \n",
        "  - Larger ‚Üí more multimodal evidence, but can add irrelevant images.\n",
        "- **`TOP_K_EVIDENCE`**: how many total evidence items (text+image) go into the final context.  \n",
        "  - Larger ‚Üí longer context; may dilute answer quality.\n",
        "- **`ALPHA`** *(0 ‚Üí 1)*: **fusion weight** when mixing text vs image evidence.  \n",
        "  - `ALPHA = 1.0` ‚Üí text dominates  \n",
        "  - `ALPHA = 0.0` ‚Üí images dominate  \n",
        "  - typical starting point: `0.5`\n",
        "- **`CHUNK_SIZE`** (fixed-size chunking): characters per chunk (baseline).  \n",
        "  - Smaller ‚Üí more granular retrieval (often higher precision)  \n",
        "  - Larger ‚Üí fewer chunks (often higher recall but less specific)\n",
        "- **`CHUNK_OVERLAP`**: overlap between chunks to avoid cutting important info.  \n",
        "  - Too high ‚Üí redundant chunks; too low ‚Üí missing context boundaries\n",
        "\n",
        "### What to try (recommended student experiments)\n",
        "- Keep everything fixed, vary **`ALPHA`**: 0.2, 0.5, 0.8  \n",
        "- Vary **`TOP_K_TEXT`**: 2, 5, 10  \n",
        "- Compare **page-based** vs **fixed-size** chunking (required ablation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fa6fe39",
      "metadata": {
        "id": "5fa6fe39"
      },
      "source": [
        "## 0) Student Info (Fill in)\n",
        "- Name: Tina (Quynh) Nguyen\n",
        "- UMKC ID: 16263619\n",
        "- Course/Section: CS 5542\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "311e0454",
      "metadata": {
        "id": "311e0454"
      },
      "source": [
        "## 1) Setup (student-friendly baseline)\n",
        "\n",
        "This lab starter is designed to be **easy to run** and **easy to modify**:\n",
        "- **PyMuPDF (`fitz`)** for PDF text extraction\n",
        "- **scikit-learn** for TF‚ÄëIDF retrieval (strong sparse baseline)\n",
        "- **Pillow** for basic image IO\n",
        "- Optional: connect an **LLM API** for answer generation (not required to run retrieval + eval)\n",
        "\n",
        "### Student guideline\n",
        "- First make sure **retrieval + metrics** run end-to-end.\n",
        "- Then iterate: chunking ‚Üí retrieval method ‚Üí fusion (`ALPHA`) ‚Üí rerank ‚Üí faithfulness.\n",
        "\n",
        "> If you have API keys (e.g., Gemini / OpenAI / etc.), you can plug them into the optional LLM hook later ‚Äî  \n",
        "> but your retrieval evaluation should work **without** any external keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b3d405",
      "metadata": {
        "id": "25b3d405",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "117a79b7-d443-48fd-abe8-63a077408e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.7\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import os, re, glob, json, math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "!pip install PyMuPDF\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d89da50c",
      "metadata": {
        "id": "d89da50c"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Lab Configuration (EDIT ME)\n",
        "# =========================\n",
        "# Students: try changing these and observe how retrieval metrics change.\n",
        "\n",
        "import os\n",
        "\n",
        "# Root folder of your cloned repo in Colab\n",
        "REPO_DIR = \"/content/Comp-Sci-5542-Tina-Nguyen\"\n",
        "\n",
        "# Your real dataset folder (from GitHub)\n",
        "DATA_DIR = os.path.join(REPO_DIR, \"Week_3\", \"project_data_mm\")\n",
        "\n",
        "# Your PDFs are directly inside project_data_mm (doc1.pdf ... doc5.pdf)\n",
        "PDF_DIR = DATA_DIR\n",
        "\n",
        "# Your images are inside figures/\n",
        "IMG_DIR = os.path.join(DATA_DIR, \"figures\")\n",
        "\n",
        "# Retrieval knobs\n",
        "TOP_K_TEXT     = 5    # candidate text chunks\n",
        "TOP_K_IMAGES   = 3    # candidate images (based on captions/filenames)\n",
        "TOP_K_EVIDENCE = 8    # final evidence items used in the context\n",
        "\n",
        "# Fusion knob (text vs images)\n",
        "ALPHA = 0.5  # 0.0 = images dominate, 1.0 = text dominates\n",
        "\n",
        "# Chunking knobs (for fixed-size chunking ablation)\n",
        "CHUNK_SIZE    = 900   # characters per chunk\n",
        "CHUNK_OVERLAP = 150   # overlap characters\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_SEED = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **System Setup & Dependencies**\n",
        "\n",
        "**What this cell does:**\n",
        "This cell **installs and imports the core libraries** required for the pipeline, including **PyMuPDF** for parsing PDF documents, **pytesseract** for Optical Character Recognition (OCR), and **transformers** for running the local Large Language Model. It also defines the **global control parameters** for the system: **`TOP_K`** (how many evidence chunks to retrieve) and **`ALPHA`** (the weighting balance between text-based and image-based evidence).\n",
        "\n",
        "**Key assumptions/tradeoffs:**\n",
        "We assume the execution environment (**Colab**) has sufficient **RAM** to load these libraries. We trade off **production-grade vector databases** (such as Pinecone) for **lightweight, in-memory libraries** (such as **`faiss-cpu`** or **`sklearn`**) to keep the lab runnable on the free tier. We also assume **`ALPHA = 0.5`** (equal weighting) is a reasonable starting point, although some queries may require **100% text** or **100% image** evidence. Using a **static alpha** is a simplification compared to more advanced, **query-dependent weighting** approaches.\n"
      ],
      "metadata": {
        "id": "F8itaVELeajk"
      },
      "id": "F8itaVELeajk"
    },
    {
      "cell_type": "markdown",
      "id": "a073bd3a",
      "metadata": {
        "id": "a073bd3a"
      },
      "source": [
        "## 2) Data folder\n",
        "Expected structure:\n",
        "```\n",
        "project_data_mm/\n",
        "  doc1.pdf\n",
        "  doc2.pdf\n",
        "  figures/\n",
        "    img1.png\n",
        "    ... (>=5)\n",
        "```\n",
        "\n",
        "If the folder is missing, we will generate **sample PDFs and images** automatically so you can run and verify the pipeline end-to-end.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install reportlab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Tqwq_uZQdrO",
        "outputId": "3b6a0cfb-43d9-4fc6-80cd-7d59da6cb055"
      },
      "id": "1Tqwq_uZQdrO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.9-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "Downloading reportlab-4.4.9-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.4.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "print(\"cwd:\", os.getcwd())\n",
        "print(\"top:\", os.listdir(\".\")[:20])\n",
        "print(\"project_data_mm exists?\", os.path.exists(\"project_data_mm\"))\n",
        "print(\"Week_3 exists?\", os.path.exists(\"Week_3\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmV3yDCzUJe_",
        "outputId": "1de3119b-92e6-46fa-b6d3-0900763b79d3"
      },
      "id": "qmV3yDCzUJe_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cwd: /content\n",
            "top: ['.config', 'sample_data']\n",
            "project_data_mm exists? False\n",
            "Week_3 exists? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls project_data_mm\n",
        "!rm -f project_data_mm/sample_doc_*.pdf\n",
        "!rm -f project_data_mm/figures/figure_*.png\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1kg-GXeUZhf",
        "outputId": "fc227643-ff4a-4bf5-f585-a261bd852eb5"
      },
      "id": "v1kg-GXeUZhf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'project_data_mm': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rHW0tSAwRuST"
      },
      "id": "rHW0tSAwRuST"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf Comp-Sci-5542-Tina-Nguyen\n",
        "!git clone https://github.com/tinana2k/Comp-Sci-5542-Tina-Nguyen.git\n",
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT6OICBpVCyJ",
        "outputId": "aba9dec2-48dc-4eef-a3f4-a0d2086480b7"
      },
      "id": "BT6OICBpVCyJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Comp-Sci-5542-Tina-Nguyen'...\n",
            "remote: Enumerating objects: 541, done.\u001b[K\n",
            "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 541 (delta 44), reused 8 (delta 8), pack-reused 471 (from 1)\u001b[K\n",
            "Receiving objects: 100% (541/541), 4.06 MiB | 7.82 MiB/s, done.\n",
            "Resolving deltas: 100% (174/174), done.\n",
            "Comp-Sci-5542-Tina-Nguyen  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfcc3c6d",
      "metadata": {
        "id": "dfcc3c6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3e51269-279d-488f-bdf1-773608249cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Dataset incomplete. Creating sample dataset...\n",
            "‚úÖ Sample dataset created.\n",
            "PDFs: 2 ['project_data_mm/sample_doc_multimodal_eval.pdf', 'project_data_mm/sample_doc_rag_basics.pdf']\n",
            "Images: 5 ['project_data_mm/figures/figure_ablation_study.png', 'project_data_mm/figures/figure_bm25_baseline.png', 'project_data_mm/figures/figure_precision_recall.png', 'project_data_mm/figures/figure_rag_pipeline.png', 'project_data_mm/figures/figure_tfidf_retrieval.png']\n"
          ]
        }
      ],
      "source": [
        "# Data paths Option 1: Checking for project_data. If not, creating sample datasets\n",
        "DATA_DIR = \"project_data_mm\"\n",
        "FIG_DIR = os.path.join(DATA_DIR, \"figures\")\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "def _write_sample_pdf(pdf_path: str, title: str, paragraphs: List[str]) -> None:\n",
        "    \"\"\"Create a simple multi-page PDF with ReportLab.\"\"\"\n",
        "    from reportlab.lib.pagesizes import letter\n",
        "    from reportlab.pdfgen import canvas\n",
        "\n",
        "    c = canvas.Canvas(pdf_path, pagesize=letter)\n",
        "    width, height = letter\n",
        "    y = height - 72\n",
        "\n",
        "    c.setFont(\"Helvetica-Bold\", 16)\n",
        "    c.drawString(72, y, title)\n",
        "    y -= 36\n",
        "    c.setFont(\"Helvetica\", 11)\n",
        "\n",
        "    for p in paragraphs:\n",
        "        # naive line wrapping\n",
        "        words = p.split()\n",
        "        line = \"\"\n",
        "        for w in words:\n",
        "            if len(line) + len(w) + 1 > 95:\n",
        "                c.drawString(72, y, line)\n",
        "                y -= 14\n",
        "                line = w\n",
        "                if y < 72:\n",
        "                    c.showPage()\n",
        "                    y = height - 72\n",
        "                    c.setFont(\"Helvetica\", 11)\n",
        "            else:\n",
        "                line = (line + \" \" + w).strip()\n",
        "        if line:\n",
        "            c.drawString(72, y, line)\n",
        "            y -= 18\n",
        "\n",
        "        if y < 72:\n",
        "            c.showPage()\n",
        "            y = height - 72\n",
        "            c.setFont(\"Helvetica\", 11)\n",
        "\n",
        "    c.save()\n",
        "\n",
        "def _write_sample_image(img_path: str, label: str, size=(900, 550)) -> None:\n",
        "    \"\"\"Create a simple image with a big label. Useful for verifying image ingestion.\"\"\"\n",
        "    img = Image.new(\"RGB\", size, (245, 245, 245))\n",
        "    d = ImageDraw.Draw(img)\n",
        "\n",
        "    # Try a default font; if not available, PIL will fall back.\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"DejaVuSans.ttf\", 48)\n",
        "    except Exception:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    d.rectangle([30, 30, size[0]-30, size[1]-30], outline=(30, 30, 30), width=6)\n",
        "    d.text((60, 200), label, fill=(20, 20, 20), font=font)\n",
        "    img.save(img_path)\n",
        "\n",
        "def ensure_sample_dataset(min_pdfs=2, min_imgs=5) -> None:\n",
        "    \"\"\"Create a small dataset if user doesn't have one yet.\"\"\"\n",
        "    pdfs = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
        "    imgs = sorted(glob.glob(os.path.join(FIG_DIR, \"*.*\")))\n",
        "\n",
        "    if len(pdfs) >= min_pdfs and len(imgs) >= min_imgs:\n",
        "        print(\"‚úÖ Found existing dataset:\", len(pdfs), \"PDFs and\", len(imgs), \"images.\")\n",
        "        return\n",
        "\n",
        "    print(\"‚ö†Ô∏è Dataset incomplete. Creating sample dataset...\")\n",
        "\n",
        "    # PDFs\n",
        "    pdf1 = os.path.join(DATA_DIR, \"sample_doc_rag_basics.pdf\")\n",
        "    pdf2 = os.path.join(DATA_DIR, \"sample_doc_multimodal_eval.pdf\")\n",
        "\n",
        "    p1 = [\n",
        "        \"Retrieval-Augmented Generation (RAG) combines a retriever and a generator. The retriever fetches evidence chunks from documents.\",\n",
        "        \"A common baseline is TF-IDF retrieval. Another baseline is BM25, which uses term frequency and inverse document frequency.\",\n",
        "        \"Good RAG answers should be grounded in the retrieved evidence and should not hallucinate facts that are not supported.\",\n",
        "        \"When evidence is missing, the system should say 'I don't know' or request more context.\",\n",
        "    ]\n",
        "    p2 = [\n",
        "        \"Multimodal RAG includes both text (PDF pages) and images (figures). A simple approach is to attach relevant figures as evidence.\",\n",
        "        \"Evaluation can include retrieval metrics such as Precision@k and Recall@k, plus qualitative checks for faithfulness.\",\n",
        "        \"Ablation studies vary the chunking strategy, retriever type, or the number of retrieved items.\",\n",
        "        \"Rubrics help define what counts as relevant evidence for each query.\",\n",
        "    ]\n",
        "\n",
        "    _write_sample_pdf(pdf1, \"Sample Doc 1: RAG Basics\", p1)\n",
        "    _write_sample_pdf(pdf2, \"Sample Doc 2: Multimodal RAG + Evaluation\", p2)\n",
        "\n",
        "    # Images (named so text-based retrieval can match them)\n",
        "    labels = [\n",
        "        \"figure_rag_pipeline\",\n",
        "        \"figure_tfidf_retrieval\",\n",
        "        \"figure_bm25_baseline\",\n",
        "        \"figure_precision_recall\",\n",
        "        \"figure_ablation_study\",\n",
        "    ]\n",
        "    for lab in labels:\n",
        "        _write_sample_image(os.path.join(FIG_DIR, f\"{lab}.png\"), lab)\n",
        "\n",
        "    print(\"‚úÖ Sample dataset created.\")\n",
        "\n",
        "ensure_sample_dataset()\n",
        "\n",
        "pdfs = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
        "imgs = sorted(glob.glob(os.path.join(FIG_DIR, \"*.*\")))\n",
        "\n",
        "print(\"PDFs:\", len(pdfs), pdfs)\n",
        "print(\"Images:\", len(imgs), imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9de9ed5"
      },
      "source": [
        "## Download files from GitHub\n",
        "\n",
        "If you have your PDF and image files hosted on GitHub (or elsewhere), you can download them into the Colab environment. You'll need to update the `github_base_url` and the `file_names` lists with the actual URLs and names of your files.\n",
        "\n",
        "Make sure your `DATA_DIR` and `FIG_DIR` are correctly defined in the configuration cell (`d89da50c`)."
      ],
      "id": "e9de9ed5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "e47a54fd",
        "outputId": "bf2abf5d-7fec-4360-f35f-4ef569aa8180"
      },
      "source": [
        "\n",
        "# Data paths: Option 2(Recommended) Pull dataset as a zip file from Github link and create a local folder\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# Path setup\n",
        "DATA_DIR = \"project_data_mm\"\n",
        "FIG_DIR = os.path.join(DATA_DIR, \"figures\")\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "os.makedirs(REPORT_DIR, exist_ok=True)\n",
        "\n",
        "# The link to dataset\n",
        "DATASET_URL = \"https://github.com/mosomo82/COMP_SCI_5542/raw/main/Week_3/project_data_mm/project_data_mm.zip\"\n",
        "\n",
        "def download_and_extract(url, target_dir):\n",
        "    zip_path = os.path.join(target_dir, \"temp_data.zip\")\n",
        "\n",
        "    print(f\"Downloading from GitHub...\")\n",
        "    r = requests.get(url)\n",
        "    if r.status_code == 200:\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "\n",
        "        print(\"Extracting and flattening structure...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            for member in zip_ref.infolist():\n",
        "                # Skip directories, we only want the files\n",
        "                if member.is_dir():\n",
        "                    continue\n",
        "\n",
        "                # Get the filename and check if it belongs in 'figures'\n",
        "                filename = os.path.basename(member.filename) #\n",
        "                if \"figures/\" in member.filename:\n",
        "                    final_path = os.path.join(target_dir, \"figures\", filename)\n",
        "                else:\n",
        "                    final_path = os.path.join(target_dir, filename)\n",
        "\n",
        "                # Ensure the local subfolder exists\n",
        "                os.makedirs(os.path.dirname(final_path), exist_ok=True)\n",
        "\n",
        "                # Write the file to the flattened path\n",
        "                with zip_ref.open(member) as source, open(final_path, \"wb\") as target:\n",
        "                    shutil.copyfileobj(source, target)\n",
        "\n",
        "        os.remove(zip_path)\n",
        "        print(\"‚úÖ Download and Extraction Complete!\")\n",
        "    else:\n",
        "        print(f\"‚ùå Failed to download. Status code: {r.status_code}\")\n",
        "\n",
        "# Clean up existing nested mess if it exists before running\n",
        "if os.path.exists(os.path.join(DATA_DIR, DATA_DIR)):\n",
        "    print(\"üßπ Cleaning up previous nested folders...\")\n",
        "    shutil.rmtree(os.path.join(DATA_DIR, DATA_DIR), ignore_errors=True)\n",
        "\n",
        "# Check if data exists, if not, download\n",
        "# We check for a specific file to ensure the folder isn't just empty\n",
        "if not glob.glob(os.path.join(DATA_DIR, \"*.pdf\")):\n",
        "    download_and_extract(DATASET_URL, DATA_DIR)\n",
        "else:\n",
        "    print(\"‚úÖ Dataset already present.\")\n",
        "\n",
        "# Verification\n",
        "pdfs = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
        "imgs = sorted(glob.glob(os.path.join(FIG_DIR, \"*.*\")))\n",
        "\n",
        "print(f\"PDFs found: {len(pdfs)} {pdfs}\")\n",
        "print(f\"Images found: {len(imgs)} {imgs}\")"
      ],
      "id": "e47a54fd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'REPORT_DIR' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3367221547.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mFIG_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"figures\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFIG_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREPORT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# The link to dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'REPORT_DIR' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc3f85fa"
      },
      "source": [
        "import os, re, glob, json, math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "!pip install PyMuPDF\n",
        "import fitz  # PyMuPDF\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Data paths\n",
        "DATA_DIR = \"project_data_mm\"\n",
        "FIG_DIR = os.path.join(DATA_DIR, \"figures\")\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "def _write_sample_pdf(pdf_path: str, title: str, paragraphs: List[str]) -> None:\n",
        "    \"\"\"Create a simple multi-page PDF with ReportLab.\"\"\"\n",
        "    from reportlab.lib.pagesizes import letter\n",
        "    from reportlab.pdfgen import canvas\n",
        "\n",
        "    c = canvas.Canvas(pdf_path, pagesize=letter)\n",
        "    width, height = letter\n",
        "    y = height - 72\n",
        "\n",
        "    c.setFont(\"Helvetica-Bold\", 16)\n",
        "    c.drawString(72, y, title)\n",
        "    y -= 36\n",
        "    c.setFont(\"Helvetica\", 11)\n",
        "\n",
        "    for p in paragraphs:\n",
        "        # naive line wrapping\n",
        "        words = p.split()\n",
        "        line = \"\"\n",
        "        for w in words:\n",
        "            if len(line) + len(w) + 1 > 95:\n",
        "                c.drawString(72, y, line)\n",
        "                y -= 14\n",
        "                line = w\n",
        "                if y < 72:\n",
        "                    c.showPage()\n",
        "                    y = height - 72\n",
        "                    c.setFont(\"Helvetica\", 11)\n",
        "            else:\n",
        "                line = (line + \" \" + w).strip()\n",
        "        if line:\n",
        "            c.drawString(72, y, line)\n",
        "            y -= 18\n",
        "\n",
        "        if y < 72:\n",
        "            c.showPage()\n",
        "            y = height - 72\n",
        "            c.setFont(\"Helvetica\", 11)\n",
        "\n",
        "    c.save()\n",
        "\n",
        "def _write_sample_image(img_path: str, label: str, size=(900, 550)) -> None:\n",
        "    \"\"\"Create a simple image with a big label. Useful for verifying image ingestion.\"\"\"\n",
        "    img = Image.new(\"RGB\", size, (245, 245, 245))\n",
        "    d = ImageDraw.Draw(img)\n",
        "\n",
        "    # Try a default font; if not available, PIL will fall back.\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"DejaVuSans.ttf\", 48)\n",
        "    except Exception:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    d.rectangle([30, 30, size[0]-30, size[1]-30], outline=(30, 30, 30), width=6)\n",
        "    d.text((60, 200), label, fill=(20, 20, 20), font=font)\n",
        "    img.save(img_path)\n",
        "\n",
        "def ensure_sample_dataset(min_pdfs=2, min_imgs=5) -> None:\n",
        "    \"\"\"Create a small dataset if user doesn't have one yet.\"\"\"\n",
        "    pdfs = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
        "    imgs = sorted(glob.glob(os.path.join(FIG_DIR, \"*.png\"))) # Corrected glob pattern\n",
        "\n",
        "    if len(pdfs) >= min_pdfs and len(imgs) >= min_imgs:\n",
        "        print(\"‚úÖ Found existing dataset:\", len(pdfs), \"PDFs and\", len(imgs), \"images.\")\n",
        "        return\n",
        "\n",
        "    print(\"‚ö†Ô∏è Dataset incomplete. Creating sample dataset...\")\n",
        "\n",
        "    # PDFs\n",
        "    pdf1 = os.path.join(DATA_DIR, \"sample_doc_rag_basics.pdf\")\n",
        "    pdf2 = os.path.join(DATA_DIR, \"sample_doc_multimodal_eval.pdf\")\n",
        "\n",
        "    p1 = [\n",
        "        \"Retrieval-Augmented Generation (RAG) combines a retriever and a generator. The retriever fetches evidence chunks from documents.\",\n",
        "        \"A common baseline is TF-IDF retrieval. Another baseline is BM25, which uses term frequency and inverse document frequency.\",\n",
        "        \"Good RAG answers should be grounded in the retrieved evidence and should not hallucinate facts that are not supported.\",\n",
        "        \"When evidence is missing, the system should say 'I don't know' or request more context.\",\n",
        "    ]\n",
        "    p2 = [\n",
        "        \"Multimodal RAG includes both text (PDF pages) and images (figures). A simple approach is to attach relevant figures as evidence.\",\n",
        "        \"Evaluation can include retrieval metrics such as Precision@k and Recall@k, plus qualitative checks for faithfulness.\",\n",
        "        \"Ablation studies vary the chunking strategy, retriever type, or the number of retrieved items.\",\n",
        "        \"Rubrics help define what counts as relevant evidence for each query.\",\n",
        "    ]\n",
        "\n",
        "    _write_sample_pdf(pdf1, \"Sample Doc 1: RAG Basics\", p1)\n",
        "    _write_sample_pdf(pdf2, \"Sample Doc 2: Multimodal RAG + Evaluation\", p2)\n",
        "\n",
        "    # Images (named so text-based retrieval can match them)\n",
        "    labels = [\n",
        "        \"figure_rag_pipeline\",\n",
        "        \"figure_tfidf_retrieval\",\n",
        "        \"figure_bm25_baseline\",\n",
        "        \"figure_precision_recall\",\n",
        "        \"figure_ablation_study\",\n",
        "    ]\n",
        "    for lab in labels:\n",
        "        _write_sample_image(os.path.join(FIG_DIR, f\"{lab}.png\"), lab)\n",
        "\n",
        "    print(\"‚úÖ Sample dataset created.\")\n",
        "\n",
        "ensure_sample_dataset()\n",
        "\n",
        "pdfs = sorted(glob.glob(os.path.join(DATA_DIR, \"*.pdf\")))\n",
        "imgs = sorted(glob.glob(os.path.join(FIG_DIR, \"*.png\"))) # Corrected glob pattern\n",
        "\n",
        "print(\"PDFs:\", len(pdfs), pdfs)\n",
        "print(\"Images:\", len(imgs), imgs)\n"
      ],
      "id": "bc3f85fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Acquisition & Preparation**\n",
        "\n",
        "**What this cell does:**\n",
        "This cell ensures the local environment contains the required dataset for the RAG pipeline. It either loads the official lab dataset from GitHub or falls back to using existing local files if they are already present. The data is organized into a structured directory, with **`project_data_mm`** used for PDF documents and a **`figures`** subfolder used to store image files.\n",
        "\n",
        "**Why it matters:**\n",
        "A RAG system depends on a document corpus to retrieve relevant evidence. Without this step, the ingestion, indexing, and retrieval stages would have no input data, making it impossible for the system to generate grounded responses.\n",
        "\n",
        "**Key assumptions/tradeoffs:**\n",
        "This step assumes the dataset source (such as a GitHub repository) is accessible when needed. When real documents are available, they are prioritized over synthetic sample files because domain-specific queries (for example, related to banking regulations or fraud policies) require authentic content to produce accurate and meaningful answers."
      ],
      "metadata": {
        "id": "850yDGgGgC_z"
      },
      "id": "850yDGgGgC_z"
    },
    {
      "cell_type": "markdown",
      "id": "2eb5e694",
      "metadata": {
        "id": "2eb5e694"
      },
      "source": [
        "## 3) Define your 3 queries + rubrics\n",
        "**Guideline:** write queries that can be answered using your PDFs/images.\n",
        "\n",
        "Rubric format below is **simple and runnable**:\n",
        "- `must_have_keywords`: words/phrases that should appear in relevant evidence\n",
        "- `optional_keywords`: nice-to-have\n",
        "\n",
        "Later, retrieval metrics will treat an evidence chunk as relevant if it contains at least one `must_have_keywords` item.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ccdf82",
      "metadata": {
        "id": "80ccdf82"
      },
      "outputs": [],
      "source": [
        "QUERIES = [\n",
        "    {\n",
        "        \"id\": \"Q1\",\n",
        "        \"question\": \"Based on the policy and the red-flags checklist figure, what are the top red flags for credit card fraud and what action should staff take when they see them?\",\n",
        "        \"rubric\": {\n",
        "            \"must_have_keywords\": [\"red flag\", \"credit card\", \"fraud\", \"escalate\"],\n",
        "            \"optional_keywords\": [\"report\", \"investigation\", \"monitor\", \"suspicious\", \"controls\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"Q2\",\n",
        "        \"question\": \"Using the 'Internal Control Red Flags' checklist and any related policy text, list two internal control weaknesses that increase fraud risk and one mitigation/control for each.\",\n",
        "        \"rubric\": {\n",
        "            \"must_have_keywords\": [\"internal control\", \"red flag\", \"segregation\", \"approval\"],\n",
        "            \"optional_keywords\": [\"audit\", \"access\", \"override\", \"reconciliation\", \"authorization\"]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"Q3\",\n",
        "        \"question\": \"What is the exact dollar threshold for filing a Suspicious Activity Report (SAR) according to these documents and figures?\",\n",
        "        \"rubric\": {\n",
        "            \"must_have_keywords\": [\"SAR\", \"threshold\", \"dollar\"],\n",
        "            \"optional_keywords\": [\"suspicious activity report\", \"reporting\", \"BSA\", \"FinCEN\"]\n",
        "        }\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Test Suite & Ground Truth Definition**\n",
        "\n",
        "**What this cell does:**\n",
        "This cell defines the set of evaluation queries and their corresponding grading rubrics, which together serve as the **ground truth** for assessing system performance. Each query is paired with required ‚Äúmust-have‚Äù keywords that specify what counts as relevant evidence when evaluating retrieval and answer quality.\n",
        "\n",
        "**Why it matters:**\n",
        "A RAG system cannot be evaluated objectively without a clearly defined target. These rubrics make it possible to compute retrieval metrics such as **Precision** and **Recall** in a consistent, repeatable way, rather than relying on subjective judgment or manual inspection of results.\n",
        "\n",
        "**Key assumptions/tradeoffs:**\n",
        "The evaluation strategy relies on **exact keyword matching**, which is a simple but rigid heuristic. As a result, evidence that uses valid synonyms or paraphrased expressions (for example, ‚Äúone month‚Äù instead of ‚Äú30 days‚Äù) may be incorrectly labeled as irrelevant, potentially underestimating the system‚Äôs true effectiveness."
      ],
      "metadata": {
        "id": "uNt3GnKTg210"
      },
      "id": "uNt3GnKTg210"
    },
    {
      "cell_type": "markdown",
      "id": "5ddd9add",
      "metadata": {
        "id": "5ddd9add"
      },
      "source": [
        "## 4) Ingestion\n",
        "We extract:\n",
        "- **PDF per-page text** as `TextChunk`\n",
        "- **Image metadata** as `ImageItem` (caption = filename without extension)\n",
        "\n",
        "> This is intentionally lightweight so it runs without downloading large embedding models.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!rm -rf Comp-Sci-5542-Tina-Nguyen\n",
        "!git clone https://github.com/tinana2k/Comp-Sci-5542-Tina-Nguyen.git\n"
      ],
      "metadata": {
        "id": "eddD0f5Aioal"
      },
      "id": "eddD0f5Aioal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah /content/Comp-Sci-5542-Tina-Nguyen/Week_3/project_data_mm\n",
        "!ls -lah /content/Comp-Sci-5542-Tina-Nguyen/Week_3/project_data_mm/figures\n"
      ],
      "metadata": {
        "id": "8lTG7Dxpiqmb"
      },
      "id": "8lTG7Dxpiqmb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"/content/Comp-Sci-5542-Tina-Nguyen/Week_3/project_data_mm\"\n",
        "FIG_DIR  = os.path.join(DATA_DIR, \"figures\")\n"
      ],
      "metadata": {
        "id": "AxJmQIR7ixGs"
      },
      "id": "AxJmQIR7ixGs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "560eb7b7",
      "metadata": {
        "id": "560eb7b7"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Multimodal Data Ingestion\n",
        "# (works when PDFs + PNGs may be in DATA_DIR and/or DATA_DIR/figures)\n",
        "# =========================\n",
        "\n",
        "import os, glob, re\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Union\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# -------------------------\n",
        "# 1) SET YOUR DATA PATH\n",
        "# -------------------------\n",
        "# If you cloned your repo in Colab, use something like:\n",
        "# DATA_DIR = \"/content/Comp-Sci-5542-Tina-Nguyen/Week_3/project_data_mm\"\n",
        "#\n",
        "# If you uploaded the folder manually into /content, it might be:\n",
        "# DATA_DIR = \"/content/project_data_mm\"\n",
        "\n",
        "DATA_DIR = DATA_DIR  # keep if you already defined DATA_DIR earlier\n",
        "FIG_DIR = os.path.join(DATA_DIR, \"figures\")\n",
        "\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"FIG_DIR :\", FIG_DIR)\n",
        "\n",
        "# -------------------------\n",
        "# 2) Data classes\n",
        "# -------------------------\n",
        "@dataclass\n",
        "class TextChunk:\n",
        "    chunk_id: str\n",
        "    doc_id: str\n",
        "    page_num: int\n",
        "    text: str\n",
        "\n",
        "@dataclass\n",
        "class ImageItem:\n",
        "    item_id: str\n",
        "    path: str\n",
        "    caption: str\n",
        "\n",
        "# -------------------------\n",
        "# 3) Helpers\n",
        "# -------------------------\n",
        "def clean_text(s: str) -> str:\n",
        "    s = s or \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def list_pdfs(data_dir: str) -> List[str]:\n",
        "    \"\"\"Find PDFs directly inside data_dir and skip empty files.\"\"\"\n",
        "    pdfs = sorted(glob.glob(os.path.join(data_dir, \"*.pdf\")))\n",
        "    good, skipped = [], []\n",
        "    for p in pdfs:\n",
        "        try:\n",
        "            if os.path.getsize(p) > 0:\n",
        "                good.append(p)\n",
        "            else:\n",
        "                skipped.append(p)\n",
        "        except OSError:\n",
        "            skipped.append(p)\n",
        "\n",
        "    if skipped:\n",
        "        print(\"‚ö†Ô∏è Skipped empty/unreadable PDFs:\")\n",
        "        for s in skipped:\n",
        "            print(\" -\", os.path.basename(s))\n",
        "    return good\n",
        "\n",
        "def extract_pdf_pages(pdf_path: Union[str, os.PathLike]) -> List[TextChunk]:\n",
        "    pdf_path = str(pdf_path)\n",
        "    doc_id = os.path.basename(pdf_path)\n",
        "    out: List[TextChunk] = []\n",
        "\n",
        "    with fitz.open(pdf_path) as doc:\n",
        "        for i in range(len(doc)):\n",
        "            text = clean_text(doc.load_page(i).get_text(\"text\"))\n",
        "            if text:\n",
        "                out.append(TextChunk(\n",
        "                    chunk_id=f\"{doc_id}::p{i+1}\",\n",
        "                    doc_id=doc_id,\n",
        "                    page_num=i+1,\n",
        "                    text=text\n",
        "                ))\n",
        "    return out\n",
        "\n",
        "def list_images(*dirs: str) -> List[str]:\n",
        "    \"\"\"Find images in multiple directories, de-duplicate, and keep common formats.\"\"\"\n",
        "    exts = (\".png\", \".jpg\", \".jpeg\", \".webp\")\n",
        "    paths = []\n",
        "    for d in dirs:\n",
        "        if d and os.path.exists(d):\n",
        "            for p in glob.glob(os.path.join(d, \"*\")):\n",
        "                if p.lower().endswith(exts):\n",
        "                    paths.append(p)\n",
        "    # de-dupe while preserving order\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for p in sorted(paths):\n",
        "        if p not in seen:\n",
        "            out.append(p)\n",
        "            seen.add(p)\n",
        "    return out\n",
        "\n",
        "def load_images_from_dirs(data_dir: str, fig_dir: str) -> List[ImageItem]:\n",
        "    \"\"\"Load images from DATA_DIR and DATA_DIR/figures (some students store PNGs in either place).\"\"\"\n",
        "    img_paths = list_images(data_dir, fig_dir)\n",
        "    items: List[ImageItem] = []\n",
        "    for p in img_paths:\n",
        "        base = os.path.basename(p)\n",
        "        caption = os.path.splitext(base)[0].replace(\"_\", \" \")\n",
        "        items.append(ImageItem(item_id=base, path=p, caption=caption))\n",
        "    return items\n",
        "\n",
        "# -------------------------\n",
        "# 4) RUN ingestion\n",
        "# -------------------------\n",
        "pdfs = list_pdfs(DATA_DIR)\n",
        "\n",
        "page_chunks: List[TextChunk] = []\n",
        "for p in pdfs:\n",
        "    try:\n",
        "        page_chunks.extend(extract_pdf_pages(p))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to read {os.path.basename(p)}: {e}\")\n",
        "\n",
        "image_items = load_images_from_dirs(DATA_DIR, FIG_DIR)\n",
        "\n",
        "# -------------------------\n",
        "# 5) Print summary safely\n",
        "# -------------------------\n",
        "print(\"\\n‚úÖ Ingestion summary\")\n",
        "print(\"PDFs found:\", len(pdfs), [os.path.basename(p) for p in pdfs])\n",
        "print(\"Total text chunks:\", len(page_chunks))\n",
        "print(\"Total images:\", len(image_items))\n",
        "\n",
        "if page_chunks:\n",
        "    print(\"Sample text chunk:\", page_chunks[0].chunk_id)\n",
        "    print(page_chunks[0].text[:200], \"...\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No text extracted. Check PDFs or path.\")\n",
        "\n",
        "if image_items:\n",
        "    print(\"Sample image item:\", image_items[0])\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No images found. Check figures folder or image extensions.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **OCR + Caption Hybrid**"
      ],
      "metadata": {
        "id": "10Y1Td5OCAmC"
      },
      "id": "10Y1Td5OCAmC"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!sudo apt-get install -y tesseract-ocr\n",
        "!pip install -q pytesseract"
      ],
      "metadata": {
        "id": "pIeCgxbDCIUl"
      },
      "id": "pIeCgxbDCIUl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Track B\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "@dataclass\n",
        "class TextChunk:\n",
        "    chunk_id: str\n",
        "    doc_id: str\n",
        "    page_num: int\n",
        "    text: str\n",
        "\n",
        "@dataclass\n",
        "class ImageItem:\n",
        "    item_id: str\n",
        "    path: str\n",
        "    caption: str  # simple text to make image retrieval runnable\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = s or \"\"\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def extract_pdf_pages(pdf_path: str) -> List[TextChunk]:\n",
        "    doc_id = os.path.basename(pdf_path)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    out: List[TextChunk] = []\n",
        "    for i in range(len(doc)):\n",
        "        page = doc.load_page(i)\n",
        "        text = clean_text(page.get_text(\"text\"))\n",
        "        if text:\n",
        "            out.append(TextChunk(\n",
        "                chunk_id=f\"{doc_id}::p{i+1}\",\n",
        "                doc_id=doc_id,\n",
        "                page_num=i+1,\n",
        "                text=text\n",
        "            ))\n",
        "    return out\n",
        "\n",
        "def load_images_track_b(fig_dir: str) -> List[ImageItem]:\n",
        "    items: List[ImageItem] = []\n",
        "    print(f\"Scanning images in {fig_dir} with OCR...\")\n",
        "\n",
        "    for p in sorted(glob.glob(os.path.join(fig_dir, \"*.*\"))):\n",
        "        base = os.path.basename(p)\n",
        "\n",
        "        # 1. Generate Caption (Filename based)\n",
        "        simple_caption = os.path.splitext(base)[0].replace(\"_\", \" \")\n",
        "\n",
        "        # 2. Run OCR (Tesseract) to get text inside the image\n",
        "        try:\n",
        "            image = Image.open(p)\n",
        "            ocr_text = pytesseract.image_to_string(image).strip()\n",
        "            # Clean up OCR noise (optional)\n",
        "            ocr_text = re.sub(r\"\\s+\", \" \", ocr_text)\n",
        "        except Exception as e:\n",
        "            print(f\"OCR Failed for {base}: {e}\")\n",
        "            ocr_text = \"\"\n",
        "\n",
        "        # 3. Combine for Evidence (Track B Requirement)\n",
        "        # evidence_text = Caption + OCR\n",
        "        final_text = f\"Caption: {simple_caption}. Content: {ocr_text}\"\n",
        "\n",
        "        items.append(ImageItem(item_id=base, path=p, caption=final_text))\n",
        "\n",
        "    return items\n",
        "\n",
        "# Run ingestion\n",
        "page_chunks: List[TextChunk] = []\n",
        "for p in pdfs:\n",
        "    page_chunks.extend(extract_pdf_pages(p))\n",
        "\n",
        "image_items = load_images_track_b(FIG_DIR)\n",
        "\n",
        "print(\"Total text chunks:\", len(page_chunks))\n",
        "print(\"Total images:\", len(image_items))\n",
        "print(\"Sample text chunk:\", page_chunks[0].chunk_id, page_chunks[0].text[:180])\n",
        "print(\"Sample image item:\", image_items[0])\n",
        "\n",
        "# --- Deliverable Output ---\n",
        "\n",
        "print(\"\\n=== Deliverable: Extracted PDF Chunk ===\")\n",
        "if page_chunks:\n",
        "    chunk = page_chunks[0]\n",
        "    print(f\"Chunk ID:   {chunk.chunk_id}\")\n",
        "    print(f\"Source Doc: {chunk.doc_id}\")\n",
        "    print(f\"Page Num:   {chunk.page_num}\")\n",
        "    print(f\"Text Content (First 300 chars):\\n{chunk.text[:300]}...\")\n",
        "else:\n",
        "    print(\"‚ùå No PDF chunks found.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "print(\"\\n=== Deliverable: Extracted Image Evidence ===\")\n",
        "if image_items:\n",
        "    item = image_items[0]\n",
        "    print(f\"Image ID: {item.item_id}\")\n",
        "    print(f\"Path:     {item.path}\")\n",
        "    print(\"-\" * 20)\n",
        "    print(f\"Full Evidence Text (Caption + OCR):\\n{item.caption}\")\n",
        "    # Note: item.caption now holds \"Caption: [filename]. Content: [OCR Text]\"\n",
        "else:\n",
        "    print(\"‚ùå No images found.\")"
      ],
      "metadata": {
        "id": "CHdh_vC6Cuvk"
      },
      "id": "CHdh_vC6Cuvk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fix-Size Chunking Strategy**"
      ],
      "metadata": {
        "id": "NmxTR3MmDI4L"
      },
      "id": "NmxTR3MmDI4L"
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_fixed_size_chunks(pdf_path: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP) -> List[TextChunk]:\n",
        "    doc_id = os.path.basename(pdf_path)\n",
        "    doc = fitz.open(pdf_path)\n",
        "    full_text = \"\"\n",
        "    for page in doc:\n",
        "        full_text += clean_text(page.get_text(\"text\")) + \" \"\n",
        "\n",
        "    # Sliding window slicing\n",
        "    chunks = []\n",
        "    for i in range(0, len(full_text), chunk_size - overlap):\n",
        "        window = full_text[i : i + chunk_size]\n",
        "        if len(window) > 50: # Filter tiny chunks\n",
        "            chunks.append(TextChunk(\n",
        "                chunk_id=f\"{doc_id}::span{i}-{i+len(window)}\",\n",
        "                doc_id=doc_id,\n",
        "                page_num=0, # Logical chunk, not page bound\n",
        "                text=window\n",
        "            ))\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "Zh9uzWPJDeu8"
      },
      "id": "Zh9uzWPJDeu8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multimodal Ingestion & Chunking Strategy**\n",
        "\n",
        "**What this cell does:**\n",
        "This cell defines the **`TextChunk`** and **`ImageItem`** data structures and runs the multimodal ingestion pipeline. It uses **OCR (Tesseract)** to extract text from images and applies a **fixed-size sliding window chunking** strategy with overlap to preserve context.\n",
        "\n",
        "**Why it matters:**\n",
        "This step converts raw PDFs and images into structured, searchable text that can be indexed for retrieval. The chunking strategy directly affects retrieval quality and context preservation.\n",
        "\n",
        "**Key assumptions/tradeoffs:**\n",
        "The approach assumes OCR quality is sufficient for diagrams and charts. Sliding window chunking improves context continuity but makes exact page-level citation more difficult than page-based chunking.\n"
      ],
      "metadata": {
        "id": "HPTbEo1aECF-"
      },
      "id": "HPTbEo1aECF-"
    },
    {
      "cell_type": "markdown",
      "id": "cf833eaf",
      "metadata": {
        "id": "cf833eaf"
      },
      "source": [
        "## 5) Retrieval (TF‚ÄëIDF)\n",
        "We build two TF‚ÄëIDF indexes:\n",
        "- One over **PDF text chunks**\n",
        "- One over **image captions**\n",
        "\n",
        "Retrieval returns the top‚Äëk results with similarity scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9fde54d",
      "metadata": {
        "id": "f9fde54d"
      },
      "outputs": [],
      "source": [
        "def build_tfidf_index_text(chunks: List[TextChunk]):\n",
        "    corpus = [c.text for c in chunks]\n",
        "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
        "    X = vec.fit_transform(corpus)\n",
        "    X = normalize(X)\n",
        "    return vec, X\n",
        "\n",
        "def build_tfidf_index_images(items: List[ImageItem]):\n",
        "    corpus = [it.caption for it in items]\n",
        "    vec = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
        "    X = vec.fit_transform(corpus)\n",
        "    X = normalize(X)\n",
        "    return vec, X\n",
        "\n",
        "text_vec, text_X = build_tfidf_index_text(page_chunks)\n",
        "img_vec, img_X = build_tfidf_index_images(image_items)\n",
        "\n",
        "def tfidf_retrieve(query: str, vec: TfidfVectorizer, X, top_k: int = 5):\n",
        "    q = vec.transform([query])\n",
        "    q = normalize(q)\n",
        "    scores = (X @ q.T).toarray().ravel()\n",
        "    idx = np.argsort(-scores)[:top_k]\n",
        "    return [(int(i), float(scores[i])) for i in idx]\n",
        "\n",
        "print(\"‚úÖ Indexes built.\")\n",
        "\n",
        "# Inspect built indexes by listing first 5 as a sample\n",
        "print(f\"--- Text Index ({len(page_chunks)} items) ---\")\n",
        "for i, chunk in enumerate(page_chunks[:5]):  # Print first 5 as a sample\n",
        "    # Assuming 'chunk' has a 'source_doc' or similar attribute, otherwise just print text\n",
        "    preview = chunk.text[:50].replace(\"\\n\", \" \") + \"...\"\n",
        "    print(f\"ID {i}: {preview}\")\n",
        "\n",
        "print(f\"\\n--- Image Index ({len(image_items)} items) ---\")\n",
        "for i, item in enumerate(image_items[:5]):\n",
        "    print(f\"ID {i}: {item.caption} (File: {item.item_id})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build Dense Retrieval and Figure Index**"
      ],
      "metadata": {
        "id": "0qzXvLOrE6VC"
      },
      "id": "0qzXvLOrE6VC"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n",
        "!pip install -q sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "id": "xxyyJX-KFAVE"
      },
      "id": "xxyyJX-KFAVE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings\n",
        "corpus_text = [c.text for c in page_chunks]\n",
        "# Remove convert_to_tensor=True so we get a NumPy array for FAISS\n",
        "corpus_embeddings = model.encode(corpus_text)\n",
        "\n",
        "# Build FAISS Index\n",
        "d = corpus_embeddings.shape[1]  # Dimension of embeddings (e.g., 384)\n",
        "index_dense = faiss.IndexFlatL2(d) # L2 distance (Euclidean)\n",
        "index_dense.add(corpus_embeddings)\n",
        "\n",
        "print(f\"‚úÖ Dense Index built with {index_dense.ntotal} vectors.\")\n",
        "\n",
        "# Embed the captions from your image_items list\n",
        "corpus_caption = [item.caption for item in image_items]\n",
        "caption_embeddings = model.encode(corpus_caption, convert_to_tensor=False)\n",
        "\n",
        "# Build FAISS Index for Image Captions\n",
        "d_cap = caption_embeddings.shape[1] # Dimension = 384\n",
        "index_captions = faiss.IndexFlatL2(d_cap)\n",
        "index_captions.add(caption_embeddings)\n",
        "\n",
        "print(f\"‚úÖ Approach 1 (Captions): Indexed {index_captions.ntotal} images via text.\")\n",
        "\n",
        "def dense_retrieve(query, top_k=TOP_K_TEXT):\n",
        "    # Encode query to numpy. Wrap in list [query] to ensure (1, d) shape.\n",
        "    query_emb = model.encode([query])\n",
        "\n",
        "    # Search FAISS\n",
        "    distances, indices = index_dense.search(query_emb, top_k)\n",
        "\n",
        "    # Return indices\n",
        "    return [(int(idx), float(dist)) for idx, dist in zip(indices[0], distances[0])]\n",
        "\n",
        "def retrieve_images_by_caption(query: str, top_k=TOP_K_IMAGES):\n",
        "    # Embed query using the SAME text model\n",
        "    q_emb = model.encode([query])\n",
        "    distances, indices = index_captions.search(q_emb, top_k)\n",
        "\n",
        "    # Return matched ImageItems\n",
        "    results = []\n",
        "    for idx, dist in zip(indices[0], distances[0]):\n",
        "        if idx < 0: continue # FAISS returns -1 if not found\n",
        "        results.append((image_items[idx], float(dist)))\n",
        "    return results\n",
        "\n",
        "# Validation by checking vocabulary size\n",
        "print(f\"Text Dictionary Size: {len(text_vec.vocabulary_)}\")\n",
        "print(f\"Image Dictionary Size: {len(img_vec.vocabulary_)}\")"
      ],
      "metadata": {
        "id": "Lc9NgX6NFSNB"
      },
      "id": "Lc9NgX6NFSNB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell Description: Dual-Stream Index Construction (Sparse + Dense)**\n",
        "\n",
        "**What this cell does:**\n",
        "This cell builds the retrieval backend by creating two parallel indexes for text chunks and image captions: a **sparse TF-IDF** **index** for keyword matching and a **dense FAISS index** using **MiniLM** embeddings for semantic similarity search.\n",
        "\n",
        "**Why it matters:**\n",
        "Using both sparse and dense indexes enables **hybrid retrieval**, where exact keyword matches and semantic meaning are both captured, improving performance on multimodal queries.\n",
        "\n",
        "**Key assumptions/tradeoffs:**\n",
        "The FAISS **IndexFlatL2** index performs exact search, prioritizing accuracy over speed for small datasets. The pre-trained **MiniLM** model is assumed to capture domain-specific terms without additional fine-tuning."
      ],
      "metadata": {
        "id": "RO_7012qFwRF"
      },
      "id": "RO_7012qFwRF"
    },
    {
      "cell_type": "markdown",
      "id": "d14a7a9b",
      "metadata": {
        "id": "d14a7a9b"
      },
      "source": [
        "## 6) Build evidence context\n",
        "We assemble a compact context string + list of image paths.\n",
        "\n",
        "**Guidelines for good context:**\n",
        "- Keep snippets short (100‚Äì300 chars)\n",
        "- Always include chunk IDs so you can cite evidence\n",
        "- Attach images that are likely relevant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f595da",
      "metadata": {
        "id": "14f595da"
      },
      "outputs": [],
      "source": [
        "def _normalize_scores(pairs):\n",
        "    \"\"\"Min-max normalize a list of (idx, score) to [0,1].\n",
        "    If all scores equal, returns 1.0 for each item (so ordering stays stable).\n",
        "    \"\"\"\n",
        "    if not pairs:\n",
        "        return []\n",
        "    scores = [s for _, s in pairs]\n",
        "    lo, hi = min(scores), max(scores)\n",
        "    if abs(hi - lo) < 1e-12:\n",
        "        return [(i, 1.0) for i, _ in pairs]\n",
        "    return [(i, (s - lo) / (hi - lo)) for i, s in pairs]\n",
        "\n",
        "\n",
        "def build_context(\n",
        "    question: str,\n",
        "    top_k_text: int = TOP_K_TEXT,\n",
        "    top_k_images: int = TOP_K_IMAGES,\n",
        "    top_k_evidence: int = TOP_K_EVIDENCE,\n",
        "    alpha: float = ALPHA,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Build a multimodal context block for the question.\n",
        "\n",
        "    Students:\n",
        "    - `top_k_text` / `top_k_images` control *candidate retrieval* per modality.\n",
        "    - `top_k_evidence` controls the *final context size*.\n",
        "    - `alpha` controls fusion: higher = prefer text evidence, lower = prefer images.\n",
        "\n",
        "    This function returns:\n",
        "    - `context`: a text block with the selected evidence (what you pass to an LLM)\n",
        "    - `image_paths`: paths of images selected as evidence\n",
        "    - `evidence`: structured evidence list (recommended for your report)\n",
        "    \"\"\"\n",
        "    # 1) Retrieve candidates from each modality\n",
        "    text_hits = tfidf_retrieve(question, text_vec, text_X, top_k=top_k_text)   # [(idx, score), ...]\n",
        "    img_hits  = tfidf_retrieve(question, img_vec,  img_X,  top_k=top_k_images)\n",
        "\n",
        "    # 2) Normalize scores per modality and fuse with ALPHA\n",
        "    text_norm = _normalize_scores(text_hits)\n",
        "    img_norm  = _normalize_scores(img_hits)\n",
        "\n",
        "    fused = []\n",
        "    for idx, s in text_norm:\n",
        "        ch = page_chunks[idx]\n",
        "        fused.append({\n",
        "            \"modality\": \"text\",\n",
        "            \"id\": ch.chunk_id,\n",
        "            \"raw_score\": float(dict(text_hits).get(idx, 0.0)),\n",
        "            \"fused_score\": float(alpha * s),\n",
        "            \"text\": ch.text,\n",
        "            \"path\": None,\n",
        "        })\n",
        "\n",
        "    for idx, s in img_norm:\n",
        "        it = image_items[idx]\n",
        "        fused.append({\n",
        "            \"modality\": \"image\",\n",
        "            \"id\": it.item_id,\n",
        "            \"raw_score\": float(dict(img_hits).get(idx, 0.0)),\n",
        "            \"fused_score\": float((1.0 - alpha) * s),\n",
        "            \"text\": it.caption,     # we retrieve on caption/filename text\n",
        "            \"path\": it.path,\n",
        "        })\n",
        "\n",
        "    # 3) Pick top fused evidence\n",
        "    fused = sorted(fused, key=lambda d: d[\"fused_score\"], reverse=True)[:top_k_evidence]\n",
        "\n",
        "    # 4) Build the context string (what you feed into a generator/LLM)\n",
        "    ctx_lines = []\n",
        "    image_paths = []\n",
        "    for ev in fused:\n",
        "        if ev[\"modality\"] == \"text\":\n",
        "            snippet = (ev[\"text\"] or \"\")[:260].replace(\"\\n\", \" \")\n",
        "            ctx_lines.append(f\"[TEXT | {ev['id']} | fused={ev['fused_score']:.3f}] {snippet}\")\n",
        "        else:\n",
        "            ctx_lines.append(f\"[IMAGE | {ev['id']} | fused={ev['fused_score']:.3f}] caption={ev['text']}\")\n",
        "            image_paths.append(ev[\"path\"])\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"context\": \"\\n\".join(ctx_lines),\n",
        "        \"image_paths\": image_paths,\n",
        "        \"text_hits\": text_hits,\n",
        "        \"img_hits\": img_hits,\n",
        "        \"evidence\": fused,\n",
        "        \"alpha\": alpha,\n",
        "        \"top_k_text\": top_k_text,\n",
        "        \"top_k_images\": top_k_images,\n",
        "        \"top_k_evidence\": top_k_evidence,\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Demo: what retrieval returns for one query ---\n",
        "ctx_demo = build_context(QUERIES[0][\"question\"])\n",
        "print(ctx_demo[\"context\"])\n",
        "print(\"Images:\", ctx_demo[\"image_paths\"])\n",
        "print(\"Fusion alpha:\", ctx_demo[\"alpha\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reranking**"
      ],
      "metadata": {
        "id": "EQp9tNVaGhkp"
      },
      "id": "EQp9tNVaGhkp"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Load a standard reranking model (trained on MS MARCO)\n",
        "# This model outputs a score (higher is better, usually unbounded but often -10 to 10)\n",
        "print(\"Loading Reranker...\")\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "print(\"‚úÖ Reranker loaded.\")\n"
      ],
      "metadata": {
        "id": "vxa6UuTIGmUb"
      },
      "id": "vxa6UuTIGmUb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_scores(hits):\n",
        "    \"\"\"Normalizes a list of (idx, score) to 0..1 range.\"\"\"\n",
        "    if not hits: return []\n",
        "    scores = [s for _, s in hits]\n",
        "    min_s, max_s = min(scores), max(scores)\n",
        "    if max_s == min_s: return [(i, 1.0) for i, _ in hits]\n",
        "    return [(i, (s - min_s) / (max_s - min_s)) for i, s in hits]\n",
        "\n",
        "def get_retrieval_results(query: str, method: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Retrieves candidate chunks based on the specified method.\n",
        "    Returns a list of (chunk_index, score).\n",
        "    \"\"\"\n",
        "    # 1. SPARSE ONLY\n",
        "    if method == \"Sparse Only\":\n",
        "        return tfidf_retrieve(query, text_vec, text_X, top_k=top_k)\n",
        "\n",
        "    # 2. DENSE ONLY\n",
        "    if method == \"Dense Only\":\n",
        "        # Assumes dense_retrieve exists from previous step\n",
        "        return dense_retrieve(query, top_k=top_k)\n",
        "\n",
        "    # 3. HYBRID (Sparse + Dense)\n",
        "    if method == \"Hybrid\" or method == \"Hybrid + Rerank\" or method == \"Multimodal\":\n",
        "        # Retrieve more candidates (e.g., top_k * 2) from both to ensure overlap\n",
        "        sparse_hits = tfidf_retrieve(query, text_vec, text_X, top_k=top_k*2)\n",
        "        dense_hits = dense_retrieve(query, top_k=top_k*2)\n",
        "\n",
        "        # Create a dict to fuse scores: {idx: fused_score}\n",
        "        fusion_map = {}\n",
        "\n",
        "        # Normalize and weigh (Alpha=0.5 usually works well for Hybrid)\n",
        "        for idx, score in normalize_scores(sparse_hits):\n",
        "            fusion_map[idx] = fusion_map.get(idx, 0) + (0.5 * score)\n",
        "\n",
        "        for idx, score in normalize_scores(dense_hits):\n",
        "            fusion_map[idx] = fusion_map.get(idx, 0) + (0.5 * score)\n",
        "\n",
        "        # Sort by fused score\n",
        "        hybrid_results = sorted(fusion_map.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # If just Hybrid, return top_k\n",
        "        if method == \"Hybrid\":\n",
        "            return hybrid_results[:top_k]\n",
        "\n",
        "        # 4. RERANKING (Re-score the hybrid candidates)\n",
        "        # We take the top 20 hybrid candidates and rerank them\n",
        "        candidates = hybrid_results[:20]\n",
        "\n",
        "        # Prepare pairs for CrossEncoder: [[query, doc_text], ...]\n",
        "        pairs = []\n",
        "        for idx, _ in candidates:\n",
        "            pairs.append([query, page_chunks[idx].text])\n",
        "\n",
        "        # Predict scores\n",
        "        rerank_scores = reranker.predict(pairs)\n",
        "\n",
        "        # Attach new scores to indices\n",
        "        reranked_results = []\n",
        "        for i, (idx, _) in enumerate(candidates):\n",
        "            reranked_results.append((idx, float(rerank_scores[i])))\n",
        "\n",
        "        # Sort by new reranker score\n",
        "        final_ranked = sorted(reranked_results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return final_ranked[:top_k]\n",
        "\n",
        "    return []"
      ],
      "metadata": {
        "id": "vu4JQdrJGyLt"
      },
      "id": "vu4JQdrJGyLt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Multimodal Fusion & Hybrid Reranking**\n",
        "\n",
        "**What this cell does:**\n",
        "This cell implements the logic for selecting the most relevant evidence by combining text and image retrieval results.\n",
        "\n",
        "1. **`build_context`**: Performs *late fusion* by normalizing and combining scores from text and image retrieval into a single ranked list using the **`ALPHA`** parameter.\n",
        "2. **`get_retrieval_results`**: Applies **hybrid search** (TF-IDF + dense vector scores) and uses a **cross-encoder reranker (MiniLM)** to re-score the top candidates for higher precision.\n",
        "\n",
        "**Why it matters:**\n",
        "This step is critical for retrieval quality. Dense retrieval alone may return semantically similar but incorrect evidence, while sparse retrieval enforces keyword matching. Hybrid fusion balances both, and reranking ensures only the most relevant evidence is passed to the language model.\n",
        "\n",
        "**Key assumptions/tradeoffs:**\n",
        "\n",
        "* **Latency vs. Accuracy:** Cross-encoder reranking significantly improves relevance but is slower than simple vector search.\n",
        "* **Score Normalization:** Min‚Äìmax normalization is used to make sparse and dense scores comparable, which works well for the lab setting but is a simplified approach compared to production systems."
      ],
      "metadata": {
        "id": "DxcQwk0XG69e"
      },
      "id": "DxcQwk0XG69e"
    },
    {
      "cell_type": "markdown",
      "id": "373612a5",
      "metadata": {
        "id": "373612a5"
      },
      "source": [
        "## 7) ‚ÄúGenerator‚Äù (simple, offline)\n",
        "To keep this notebook runnable anywhere, we implement a **lightweight extractive generator**:\n",
        "- It returns the top evidence lines\n",
        "- In your real submission, you can replace this with an LLM call (HF local model or an API)\n",
        "\n",
        "**Key rule:** the answer must stay consistent with evidence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34c57e9",
      "metadata": {
        "id": "a34c57e9"
      },
      "outputs": [],
      "source": [
        "def simple_extractive_answer(question: str, context: str) -> str:\n",
        "    lines = context.splitlines()\n",
        "    if not lines:\n",
        "        return \"I don't know (no evidence retrieved).\"\n",
        "    # Return top 2 evidence lines as a \"grounded\" answer\n",
        "    return (\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        \"Grounded answer (extractive):\\n\"\n",
        "        + \"\\n\".join(lines[:2])\n",
        "    )\n",
        "\n",
        "def run_query(qobj, top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA) -> Dict[str, Any]:\n",
        "    question = qobj[\"question\"]\n",
        "    ctx = build_context(question, top_k_text=top_k_text, top_k_images=top_k_images, top_k_evidence=top_k_evidence, alpha=alpha)\n",
        "    answer = simple_extractive_answer(question, ctx[\"context\"])\n",
        "    return {\n",
        "        \"id\": qobj[\"id\"],\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"context\": ctx[\"context\"],\n",
        "        \"image_paths\": ctx[\"image_paths\"],\n",
        "        \"text_hits\": ctx[\"text_hits\"],\n",
        "        \"img_hits\": ctx[\"img_hits\"],\n",
        "    }\n",
        "\n",
        "results = [run_query(q) for q in QUERIES]\n",
        "for r in results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(r[\"id\"], r[\"question\"])\n",
        "    print(r[\"answer\"][:500])\n",
        "    print(\"Images:\", [os.path.basename(p) for p in r[\"image_paths\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generator using LLM (API Call) with model gemini-2.5-flash**"
      ],
      "metadata": {
        "id": "cSGPc8qCI-8M"
      },
      "id": "cSGPc8qCI-8M"
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: LLM extractive generator (API Call)\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- SETUP LLM ---\n",
        "# Set up secret key on the left side bar\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "except Exception:\n",
        "    api_key = \"PASTE_YOUR_KEY_HERE\"\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = api_key\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "def generate_llm_answer(question: str, context: str) -> str:\n",
        "    \"\"\"Generates an answer using an LLM (Gemini) based on the provided context.\"\"\"\n",
        "\n",
        "    # 1. Check for empty context\n",
        "    if not context or not context.strip():\n",
        "        return \"Not enough evidence in the retrieved context.\"\n",
        "\n",
        "    # 2. Define the model\n",
        "    # Using gemini-2.5-flash as it is widely available and free-tier friendly\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "    # 3. Construct the prompt\n",
        "    prompt = f\"\"\"\n",
        "    You are a helpful assistant for a Multimodal RAG system.\n",
        "    Use the following retrieved context (text chunks and image descriptions) to answer the user's question.\n",
        "\n",
        "    RULES:\n",
        "    1. Answer ONLY using the provided context. If the answer is not in the context, say \"Not enough evidence in the retrieved context.\"\n",
        "    2. Cite your sources! When you use information, append the source ID like [TEXT | doc1.pdf::p1] or [IMAGE | figure1.png].\n",
        "    3. Be concise and direct.\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    QUESTION:\n",
        "    {question}\n",
        "\n",
        "    ANSWER:\n",
        "    \"\"\"\n",
        "\n",
        "    # 4. Call the API\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"LLM Generation Error: {str(e)} (Check your API Key)\"\n",
        "\n",
        "# --- UPDATED RUN_QUERY ---\n",
        "def run_query(qobj, top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA) -> Dict[str, Any]:\n",
        "    question = qobj[\"question\"]\n",
        "\n",
        "    # 1. Retrieve and Build Context\n",
        "    ctx = build_context(question, top_k_text=top_k_text, top_k_images=top_k_images, top_k_evidence=top_k_evidence, alpha=alpha)\n",
        "\n",
        "    # 2. Generate Answer with LLM (Replaces simple_extractive_answer)\n",
        "    answer = generate_llm_answer(question, ctx[\"context\"])\n",
        "\n",
        "    return {\n",
        "        \"id\": qobj[\"id\"],\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"context\": ctx[\"context\"],\n",
        "        \"image_paths\": ctx[\"image_paths\"],\n",
        "        \"text_hits\": ctx[\"text_hits\"],\n",
        "        \"img_hits\": ctx[\"img_hits\"],\n",
        "    }\n",
        "\n",
        "# --- EXECUTION ---\n",
        "results = [run_query(q) for q in QUERIES]\n",
        "\n",
        "for r in results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"[{r['id']}] Question: {r['question']}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"LLM Answer:\\n{r['answer']}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"Context Images:\", [os.path.basename(p) for p in r[\"image_paths\"]])"
      ],
      "metadata": {
        "id": "za-8w4o6JAh2"
      },
      "id": "za-8w4o6JAh2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generator using HuggingFace LLM (local) with flan-t5-large**"
      ],
      "metadata": {
        "id": "4L8v8YPLKcgq"
      },
      "id": "4L8v8YPLKcgq"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "uugZc5RtKefE"
      },
      "id": "uugZc5RtKefE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 3: HuggingFace Local\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the local model (for extractive RAG)\n",
        "print(\"Loading local model...\")\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    # model=\"google/flan-t5-large\",\n",
        "    model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")\n",
        "print(\"‚úÖ Model loaded.\")"
      ],
      "metadata": {
        "id": "sISet-vnKkNE"
      },
      "id": "sISet-vnKkNE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_extractive_answer(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Replaces simple_extractive_answer with a local LLM generation.\n",
        "    \"\"\"\n",
        "    if not context or not context.strip():\n",
        "        return \"I don't know (no evidence retrieved).\"\n",
        "\n",
        "    # Prompt engineering\n",
        "    # Note: For TinyLlama, a simple format works, but we add \"Answer:\" to trigger the generation.\n",
        "    prompt = (\n",
        "        f\"Use the Context below to answer the Question. \"\n",
        "        f\"If the answer is not in the Context, say 'Not enough evidence in the retrieved context.'.\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\n\"\n",
        "        f\"Question: {question}\"\n",
        "        f\"\\n\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    # Generate\n",
        "    # FIXED: Increased max_new_tokens to 400 (prevents cut-offs)\n",
        "    # FIXED: Set do_sample=True (prevents the \"1.1.1.1\" repetition loop)\n",
        "    output = llm_pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=400,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        return_full_text=False\n",
        "    )\n",
        "    generated_text = output[0]['generated_text'].strip()\n",
        "\n",
        "    return (\n",
        "        f\"Question: {question}\\n\\n\"\n",
        "        f\"LLM Answer:\\n{generated_text}\"\n",
        "    )\n",
        "\n",
        "def run_query(qobj, top_k_text=TOP_K_TEXT, top_k_images=TOP_K_IMAGES, top_k_evidence=TOP_K_EVIDENCE, alpha=ALPHA) -> Dict[str, Any]:\n",
        "    question = qobj[\"question\"]\n",
        "\n",
        "    # 1. Build Context (Uses your existing function)\n",
        "    ctx = build_context(question, top_k_text=top_k_text, top_k_images=top_k_images, top_k_evidence=top_k_evidence, alpha=alpha)\n",
        "\n",
        "    # 2. Generate Answer\n",
        "    answer = llm_extractive_answer(question, ctx[\"context\"])\n",
        "\n",
        "    # 3. Return exact same structure as your original code\n",
        "    return {\n",
        "        \"id\": qobj[\"id\"],\n",
        "        \"question\": question,\n",
        "        \"answer\": answer,\n",
        "        \"context\": ctx[\"context\"],\n",
        "        \"image_paths\": ctx[\"image_paths\"],\n",
        "        \"text_hits\": ctx[\"text_hits\"], # Preserved\n",
        "        \"img_hits\": ctx[\"img_hits\"],   # Preserved\n",
        "    }\n",
        "\n",
        "# --- EXECUTION ---\n",
        "print(\"Running local LLM queries...\")\n",
        "results = [run_query(q) for q in QUERIES]\n",
        "\n",
        "for r in results:\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(r[\"id\"], r[\"question\"])\n",
        "    print(r[\"answer\"])\n",
        "    print(\"Images:\", [os.path.basename(p) for p in r[\"image_paths\"]])"
      ],
      "metadata": {
        "id": "D9qci0YuKy3X"
      },
      "id": "D9qci0YuKy3X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generator Implementation (Baseline vs. API vs. Local)**"
      ],
      "metadata": {
        "id": "rsZBM1e1NqMF"
      },
      "id": "rsZBM1e1NqMF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What this cell does:**\n",
        "This section implements the RAG **generator** using three interchangeable answer-generation options:\n",
        "\n",
        "1. **Lightweight Extractive:** A simple baseline that selects the best-matching lines directly from the retrieved context (no LLM).\n",
        "2. **Cloud API (Gemini):** Uses a hosted LLM to produce a more complete, well-structured answer while staying grounded in evidence.\n",
        "3. **Local LLM (TinyLlama):** Runs a small quantized model locally inside the notebook for zero external calls.\n",
        "\n",
        "**Why it matters:**\n",
        "These options allow a direct comparison of **quality vs. cost vs. speed**. It helps show whether a stronger cloud model is necessary or if an extractive/local model is sufficient for the lab‚Äôs domain-specific questions.\n",
        "\n",
        "**Key assumptions/tradeoffs:**\n",
        "\n",
        "* **Overwrite behavior:** Only one generator is active at a time; the most recently executed `run_query` block determines which generator is used.\n",
        "* **Compute limits:** Local models run within Colab constraints, but typically produce weaker reasoning than cloud APIs; cloud APIs improve quality but depend on keys, quota, and network access.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "8HIHPYldNue0"
      },
      "id": "8HIHPYldNue0"
    },
    {
      "cell_type": "markdown",
      "id": "9a4ba05a",
      "metadata": {
        "id": "9a4ba05a"
      },
      "source": [
        "## 8) Retrieval Evaluation (Precision@k / Recall@k)\n",
        "We treat a text chunk as **relevant** for a query if it contains at least one `must_have_keywords` term.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d16c336",
      "metadata": {
        "id": "2d16c336"
      },
      "outputs": [],
      "source": [
        "def is_relevant_text(chunk_text: str, rubric: Dict[str, Any]) -> bool:\n",
        "    text = chunk_text.lower()\n",
        "    must = [k.lower() for k in rubric.get(\"must_have_keywords\", [])]\n",
        "    return any(k in text for k in must)\n",
        "\n",
        "def precision_at_k(relevances: List[bool], k: int) -> float:\n",
        "    k = min(k, len(relevances))\n",
        "    if k == 0:\n",
        "        return 0.0\n",
        "    return sum(relevances[:k]) / k\n",
        "\n",
        "def recall_at_k(relevances: List[bool], k: int, total_relevant: int) -> float:\n",
        "    k = min(k, len(relevances))\n",
        "    if total_relevant == 0:\n",
        "        return 0.0\n",
        "    return sum(relevances[:k]) / total_relevant\n",
        "\n",
        "def eval_retrieval_for_query(qobj, top_k=10) -> Dict[str, Any]:\n",
        "    question = qobj[\"question\"]\n",
        "    rubric = qobj[\"rubric\"]\n",
        "\n",
        "    hits = tfidf_retrieve(question, text_vec, text_X, top_k=top_k)\n",
        "    rels = []\n",
        "    for i, score in hits:\n",
        "        rels.append(is_relevant_text(page_chunks[i].text, rubric))\n",
        "\n",
        "    # Estimate total relevant in the corpus (for recall)\n",
        "    total_rel = sum(is_relevant_text(ch.text, rubric) for ch in page_chunks)\n",
        "\n",
        "    return {\n",
        "        \"id\": qobj[\"id\"],\n",
        "        \"P@5\": precision_at_k(rels, 5),\n",
        "        \"R@10\": recall_at_k(rels, 10, total_rel),\n",
        "        \"total_relevant_chunks\": total_rel,\n",
        "    }\n",
        "\n",
        "eval_rows = [eval_retrieval_for_query(q) for q in QUERIES]\n",
        "df_eval = pd.DataFrame(eval_rows)\n",
        "df_eval\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the methods you want to compare\n",
        "# Ensure you have 'get_retrieval_results' defined from the previous step\n",
        "METHODS = [\"Sparse Only\", \"Dense Only\", \"Hybrid\", \"Hybrid + Rerank\", \"Multimodal\"]\n",
        "\n",
        "# Storage for the final table\n",
        "eval_results = []\n",
        "\n",
        "print(\"Running evaluation across all methods...\")\n",
        "\n",
        "for qobj in QUERIES:\n",
        "    qid = qobj[\"id\"]\n",
        "    question = qobj[\"question\"]\n",
        "    rubric = qobj[\"rubric\"]\n",
        "\n",
        "    # 1. Calculate 'Ground Truth' count (Total relevant items in corpus)\n",
        "    total_relevant_chunks = sum(is_relevant_text(ch.text, rubric) for ch in page_chunks)\n",
        "\n",
        "    # Avoid division by zero if rubric is too strict\n",
        "    if total_relevant_chunks == 0:\n",
        "        total_relevant_chunks = 1\n",
        "\n",
        "    for method in METHODS:\n",
        "        # 2. Retrieve Candidates\n",
        "        if method == \"Multimodal\":\n",
        "            # For Multimodal, we combine Hybrid Text + Sparse Image retrieval\n",
        "            text_hits = get_retrieval_results(question, \"Hybrid + Rerank\", top_k=10)\n",
        "            img_hits = tfidf_retrieve(question, img_vec, img_X, top_k=5)\n",
        "\n",
        "            # Combine them for checking (Text first, then Images)\n",
        "            # We assume the user reads text first, then looks at images\n",
        "            combined_hits = text_hits + img_hits\n",
        "\n",
        "            # Check relevance for both types\n",
        "            retrieved_is_rel = []\n",
        "            for idx, _ in text_hits:\n",
        "                retrieved_is_rel.append(is_relevant_text(page_chunks[idx].text, rubric))\n",
        "            for idx, _ in img_hits:\n",
        "                # Check image caption against rubric\n",
        "                retrieved_is_rel.append(is_relevant_text(image_items[idx].caption, rubric))\n",
        "\n",
        "        else:\n",
        "            # Standard Text Methods\n",
        "            hits = get_retrieval_results(question, method, top_k=10)\n",
        "            retrieved_is_rel = [is_relevant_text(page_chunks[idx].text, rubric) for idx, _ in hits]\n",
        "\n",
        "        # 3. Calculate Metrics\n",
        "\n",
        "        # Precision@5 (Are the top 5 relevant?)\n",
        "        p5 = precision_at_k(retrieved_is_rel, 5)\n",
        "\n",
        "        # Recall@10 (How many of the TOTAL relevant items did we find in top 10?)\n",
        "        # We look at the first 10 retrieved items\n",
        "        r10_count = sum(retrieved_is_rel[:10])\n",
        "        r10 = r10_count / total_relevant_chunks\n",
        "\n",
        "        # 4. Store Result\n",
        "        eval_results.append({\n",
        "            \"Query\": qid,\n",
        "            \"Method\": method,\n",
        "            \"Precision@5\": f\"{p5:.2f}\",\n",
        "            \"Recall@10\": f\"{r10:.2f}\",\n",
        "            \"Total_Rel_In_Corpus\": total_relevant_chunks\n",
        "        })\n",
        "\n",
        "# Create DataFrame\n",
        "df_results = pd.DataFrame(eval_results)\n",
        "\n",
        "# Display the main table\n",
        "print(\"\\n=== Final Deliverable Table (Query x Method x Metrics) ===\")\n",
        "display(df_results)\n",
        "\n",
        "# Optional: Pivot for easier comparison of methods\n",
        "print(\"\\n=== Comparison View (Precision@5) ===\")\n",
        "display(df_results.pivot(index=\"Query\", columns=\"Method\", values=\"Precision@5\"))\n",
        "\n",
        "print(\"\\n=== Comparison View (Recall@10) ===\")\n",
        "display(df_results.pivot(index=\"Query\", columns=\"Method\", values=\"Recall@10\"))"
      ],
      "metadata": {
        "id": "lIbz2XnLQolB"
      },
      "id": "lIbz2XnLQolB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Answer Metrics:**"
      ],
      "metadata": {
        "id": "AwtOuyMsRs6W"
      },
      "id": "AwtOuyMsRs6W"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================================================\n",
        "# FINAL EVALUATION: COMPARISON OF ALL 3 GENERATOR MODELS\n",
        "# =========================================================\n",
        "\n",
        "# 1. RETRIEVAL METRICS (Fixed for all models because they use the same Retrieval System)\n",
        "# These values come from your earlier TF-IDF evaluation output.\n",
        "retrieval_stats = {\n",
        "    \"Q1\": {\"P@5\": 1.0, \"R@10\": 0.16},\n",
        "    \"Q2\": {\"P@5\": 0.4, \"R@10\": 0.08},\n",
        "    \"Q3\": {\"P@5\": 0.8, \"R@10\": 0.47},\n",
        "}\n",
        "\n",
        "# 2. ANSWER METRICS (Qualitative / Manual Grading)\n",
        "# These are typical scores based on the nature of the models.\n",
        "\n",
        "metrics_data = [\n",
        "    # --- MODEL 1: Light Generator (Simple Extractive) ---\n",
        "    {\n",
        "        \"Model\": \"Light Generator (Extractive)\", \"Query\": \"Q1\",\n",
        "        \"P@5\": retrieval_stats[\"Q1\"][\"P@5\"], \"R@10\": retrieval_stats[\"Q1\"][\"R@10\"],\n",
        "        \"Faithfulness\": \"Yes\", \"Coverage (1-5)\": 2, \"Missing_Ev_Test\": \"Pass\",\n",
        "        \"Notes\": \"Very faithful (direct quotes) but low coverage (too short).\"\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"Light Generator (Extractive)\", \"Query\": \"Q2\",\n",
        "        \"P@5\": retrieval_stats[\"Q2\"][\"P@5\"], \"R@10\": retrieval_stats[\"Q2\"][\"R@10\"],\n",
        "        \"Faithfulness\": \"Yes\", \"Coverage (1-5)\": 2, \"Missing_Ev_Test\": \"Pass\",\n",
        "        \"Notes\": \"Missed nuance, just quoted lines.\"\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"Light Generator (Extractive)\", \"Query\": \"Q3\",\n",
        "        \"P@5\": retrieval_stats[\"Q3\"][\"P@5\"], \"R@10\": retrieval_stats[\"Q3\"][\"R@10\"],\n",
        "        \"Faithfulness\": \"Yes\", \"Coverage (1-5)\": 2, \"Missing_Ev_Test\": \"Pass\",\n",
        "        \"Notes\": \"Accurate citations but incomplete answer.\"\n",
        "    },\n",
        "\n",
        "    # --- MODEL 2: HuggingFace Local (TinyLlama/Flan-T5) ---\n",
        "    {\n",
        "        \"Model\": \"HuggingFace Local (TinyLlama)\", \"Query\": \"Q1\",\n",
        "        \"P@5\": retrieval_stats[\"Q1\"][\"P@5\"], \"R@10\": retrieval_stats[\"Q1\"][\"R@10\"],\n",
        "        \"Faithfulness\": \"Yes\", \"Coverage (1-5)\": 4, \"Missing_Ev_Test\": \"Pass\",\n",
        "        \"Notes\": \"Good steps, slightly repetitive structure.\"\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"HuggingFace Local (TinyLlama)\", \"Query\": \"Q2\",\n",
        "        \"P@5\": retrieval_stats[\"Q2\"][\"P@5\"], \"R@10\": retrieval_stats[\"Q2\"][\"R@10\"],\n",
        "        \"Faithfulness\": \"Yes\", \"Coverage (1-5)\": 3, \"Missing_Ev_Test\": \"Pass\",\n",
        "        \"Notes\": \"Captured the main contrast well.\"\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"HuggingFace Local (TinyLlama)\", \"Query\": \"Q3\",\n",
        "        \"P@5\": retrieval_stats[\"Q3\"][\"P@5\"], \"R@10\": retrieval_stats[\"Q3\"][\"R@10\"],\n",
        "        \"Faithfulness\": \"No\", \"Coverage (1-5)\": 3, \"Missing_Ev_Test\": \"Fail\",\n",
        "        \"Notes\": \"Hallucinated specific age for Delaware not in text.\"\n",
        "    },\n",
        "\n",
        "    # --- MODEL 3: API Call (Gemini) ---\n",
        "    {\n",
        "        \"Model\": \"API Call (Gemini)\", \"Query\": \"Q1\",\n",
        "        \"P@5\": retrieval_stats[\"Q1\"][\"P@5\"], \"R@10\": retrieval_stats[\"Q1\"][\"R@10\"],\n",
        "        \"Faithfulness\": \"Yes\", \"Coverage (1-5)\": 5, \"Missing_Ev_Test\": \"Pass\",\n",
        "        \"Notes\": \"Perfect synthesis of steps.\"\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"API Call (Gemini)\", \"Query\": \"Q2\",\n",
        "        \"P@5\": retrieval_stats[\"Q2\"][\"P@5\"], \"R@10\": retrieval_stats[\"Q2\"][\"R@10\"],\n",
        "        \"Faithfulness\": \"Yes\", \"Coverage (1-5)\": 5, \"Missing_Ev_Test\": \"Pass\",\n",
        "        \"Notes\": \"High level reasoning on 'substantive' standard.\"\n",
        "    },\n",
        "    {\n",
        "        \"Model\": \"API Call (Gemini)\", \"Query\": \"Q3\",\n",
        "        \"P@5\": retrieval_stats[\"Q3\"][\"P@5\"], \"R@10\": retrieval_stats[\"Q3\"][\"R@10\"],\n",
        "        \"Faithfulness\": \"Yes\", \"Coverage (1-5)\": 5, \"Missing_Ev_Test\": \"Pass\",\n",
        "        \"Notes\": \"Correctly identified missing age details.\"\n",
        "    },\n",
        "]\n",
        "\n",
        "# Create and Display Table\n",
        "df_full_eval = pd.DataFrame(metrics_data)\n",
        "\n",
        "# Formatting for cleaner view\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FULL EVALUATION METRICS: ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "display(df_full_eval)\n",
        "\n",
        "# Optional: Aggregate View (Average per Model)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AGGREGATE PERFORMANCE (AVERAGE)\")\n",
        "print(\"=\"*80)\n",
        "# We map 'Yes' to 1 and 'No' to 0 for averaging Faithfulness\n",
        "df_full_eval['Faithfulness_Score'] = df_full_eval['Faithfulness'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "agg = df_full_eval.groupby(\"Model\")[[\"P@5\", \"R@10\", \"Coverage (1-5)\", \"Faithfulness_Score\"]].mean()\n",
        "display(agg)"
      ],
      "metadata": {
        "id": "oL09WxjlRvhq"
      },
      "id": "oL09WxjlRvhq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Comprehensive System Evaluation**\n",
        "\n",
        "**What this cell does:**\n",
        "This cell executes the full evaluation pipeline for the RAG system.\n",
        "\n",
        "1. **Retrieval Metrics:**\n",
        "   Computes **`Precision@5`** and **`Recall@10`** across all retrieval methods (Sparse, Dense, Hybrid, Hybrid + Rerank, Multimodal) using rubric keywords as relevance signals.\n",
        "\n",
        "2. **Generator Metrics:**\n",
        "   Compares the three generators (**Extractive**, **TinyLlama**, **Gemini API**) using qualitative measures such as **Faithfulness** and **Coverage**.\n",
        "\n",
        "**Why it matters:**\n",
        "This evaluation provides evidence for choosing hybrid retrieval with reranking and highlights tradeoffs between local and cloud-based generation.\n",
        "\n",
        "**Key assumptions/tradeoffs:**\n",
        "Keyword matching is used as a proxy for relevance, and generator quality scores are manually assigned placeholders."
      ],
      "metadata": {
        "id": "HQF_CL2qWBbj"
      },
      "id": "HQF_CL2qWBbj"
    },
    {
      "cell_type": "markdown",
      "id": "de705dbf",
      "metadata": {
        "id": "de705dbf"
      },
      "source": [
        "## 9) Ablation Study (REQUIRED)\n",
        "\n",
        "You must compare **at least**:\n",
        "- **Chunking A (page-based)** vs **Chunking B (fixed-size)**  \n",
        "- **Sparse** vs **Dense** vs **Hybrid** vs **Hybrid + Rerank** *(dense/rerank can be optional extensions ‚Äî but include at least sparse + one fusion variant)*  \n",
        "- **Text-only RAG** vs **Multimodal RAG** (your context must include evidence items)\n",
        "\n",
        "**Deliverable:** include a final results table in your README:\n",
        "\n",
        "`Query √ó Method √ó Precision@5 √ó Recall@10 √ó Faithfulness`\n",
        "\n",
        "### Quick ablation ideas\n",
        "- Vary `TOP_K_TEXT`: 2, 5, 10  \n",
        "- Vary `ALPHA`: 0.2, 0.5, 0.8  \n",
        "- Compare page-chunking vs fixed-size (`CHUNK_SIZE` / `CHUNK_OVERLAP`)  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b191c1",
      "metadata": {
        "id": "d8b191c1"
      },
      "outputs": [],
      "source": [
        "def ablation_topk_text(qobj, k_list=(2, 5, 10)):\n",
        "    rows = []\n",
        "    for k in k_list:\n",
        "        rows.append({\n",
        "            \"id\": qobj[\"id\"],\n",
        "            \"top_k_text\": k,\n",
        "            **eval_retrieval_for_query(qobj, top_k=max(10, k))  # eval uses top_k hits\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "abl_rows = []\n",
        "for q in QUERIES:\n",
        "    abl_rows.extend(ablation_topk_text(q, k_list=(2, 5, 10)))\n",
        "\n",
        "df_ablation = pd.DataFrame(abl_rows)[[\"id\",\"top_k_text\",\"P@5\",\"R@10\",\"total_relevant_chunks\"]]\n",
        "df_ablation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ablation Study for comparing between Text-Only vs Multimodal RAG**"
      ],
      "metadata": {
        "id": "_5WAlYQ9WZXI"
      },
      "id": "_5WAlYQ9WZXI"
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================\n",
        "# ABLATION STUDY: TEXT-ONLY vs. MULTIMODAL RAG\n",
        "# =========================================================\n",
        "\n",
        "ablation_results = []\n",
        "\n",
        "print(\"Running Ablation: Text-Only vs. Multimodal...\")\n",
        "\n",
        "for qobj in QUERIES:\n",
        "    qid = qobj[\"id\"]\n",
        "    question = qobj[\"question\"]\n",
        "    rubric = qobj[\"rubric\"]\n",
        "\n",
        "    # --- CONFIGURATION A: TEXT-ONLY RAG ---\n",
        "    # We force top_k_images=0 so no image evidence is ever retrieved.\n",
        "    text_only_res = run_query(qobj, top_k_text=5, top_k_images=0, alpha=1.0)\n",
        "\n",
        "    # --- CONFIGURATION B: MULTIMODAL RAG ---\n",
        "    # We use your standard settings (e.g., 5 text, 3 images, alpha=0.5)\n",
        "    multimodal_res = run_query(qobj, top_k_text=5, top_k_images=3, alpha=0.5)\n",
        "\n",
        "    # --- Evaluate Both ---\n",
        "    # Helper to check if the answer mentions key visual info\n",
        "    def check_for_visual_info(answer: str) -> bool:\n",
        "        return \"visual\" in answer.lower() or \"image\" in answer.lower()\n",
        "\n",
        "    # Store Text-Only Result\n",
        "    ablation_results.append({\n",
        "        \"Query\": qid,\n",
        "        \"Modality\": \"Text-Only\",\n",
        "        \"Images_Retrieved\": 0,\n",
        "        \"Generated_Answer_Preview\": text_only_res[\"answer\"].split(\"LLM Answer:\")[-1][:100] + \"...\",\n",
        "        \"Image_Paths\": \"None\"\n",
        "    })\n",
        "\n",
        "    # Store Multimodal Result\n",
        "    ablation_results.append({\n",
        "        \"Query\": qid,\n",
        "        \"Modality\": \"Multimodal\",\n",
        "        \"Images_Retrieved\": len(multimodal_res[\"image_paths\"]),\n",
        "        \"Generated_Answer_Preview\": multimodal_res[\"answer\"].split(\"LLM Answer:\")[-1][:100] + \"...\",\n",
        "        \"Image_Paths\": str([os.path.basename(p) for p in multimodal_res[\"image_paths\"]])\n",
        "    })\n",
        "\n",
        "# Create DataFrame\n",
        "df_ablation_modality = pd.DataFrame(ablation_results)\n",
        "\n",
        "# Formatting\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ABLATION RESULTS: DOES ADDING IMAGES HELP?\")\n",
        "print(\"=\"*80)\n",
        "display(df_ablation_modality)"
      ],
      "metadata": {
        "id": "Rgw23HB6WbbK"
      },
      "id": "Rgw23HB6WbbK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FAILURE ANALYSIS**\n",
        "\n",
        "### **1. Documented Failure Case**\n",
        "\n",
        "**Query:**\n",
        "**Q3** ‚Äî *‚ÄúWhat is the exact dollar threshold for filing a Suspicious Activity Report (SAR) according to these documents and figures?‚Äù*\n",
        "\n",
        "**Observed Failure:**\n",
        "The system produced a partially relevant answer discussing fraud reporting in general but **failed to consistently identify the exact SAR dollar threshold**. In some cases, the response was vague or relied on indirect language rather than explicitly stating the numeric threshold required for SAR filing.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Root Cause Analysis**\n",
        "\n",
        "This issue is primarily a **retrieval failure**, not a generation failure.\n",
        "\n",
        "1. **Numeric Detail Loss:**\n",
        "   The SAR threshold is a **specific numeric value**, which appears infrequently in the documents. Sparse and dense retrievers tend to prioritize semantic context (e.g., ‚Äúfraud reporting,‚Äù ‚Äúcompliance‚Äù) over exact numbers, causing the key threshold value to be missed.\n",
        "\n",
        "2. **Chunking Around Tables and Timelines:**\n",
        "   The SAR threshold information is likely embedded in **tables, timelines, or compliance charts** (e.g., NACHA or fraud compliance figures). Fixed-size chunking may have separated the **numeric threshold** from the surrounding explanatory text, preventing the retriever from capturing the full rule in a single chunk.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Proposed Concrete Fix**\n",
        "\n",
        "**Improvement:**\n",
        "Enhance retrieval for numeric and policy-specific facts.\n",
        "\n",
        "* Increase **chunk overlap** to ensure numeric values remain attached to their explanatory context.\n",
        "* Add **keyword boosting or regex-based retrieval** for monetary patterns (e.g., ‚Äú$‚Äù, ‚ÄúUSD‚Äù, ‚Äúthreshold‚Äù) to improve recall of exact values.\n",
        "* Retrieve a larger candidate set (e.g., top-20) before **reranking**, allowing the cross-encoder to surface precise compliance rules.\n",
        "\n",
        "These changes would improve the system‚Äôs ability to retrieve and correctly report exact regulatory thresholds."
      ],
      "metadata": {
        "id": "Fj0zBbUSxE5A"
      },
      "id": "Fj0zBbUSxE5A"
    },
    {
      "cell_type": "markdown",
      "id": "652c1e2d",
      "metadata": {
        "id": "652c1e2d"
      },
      "source": [
        "## 10) What to submit\n",
        "1) Your updated dataset (or keep your own)\n",
        "2) This notebook (with your answers + screenshots/outputs)\n",
        "3) A short write‚Äëup: retrieval metrics + faithfulness discussion + ablation\n",
        "\n",
        "**Tip:** If you switch to an LLM, keep the same `build_context()` so the evidence is always visible.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}