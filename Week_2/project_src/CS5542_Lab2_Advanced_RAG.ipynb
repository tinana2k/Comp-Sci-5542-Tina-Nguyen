{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinana2k/Comp-Sci-5542-Tina-Nguyen/blob/main/CS5542_Lab2_Advanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ef0856",
      "metadata": {
        "id": "04ef0856"
      },
      "source": [
        "# CS 5542 — Lab 2: Advanced RAG Systems Engineering (Revised Notebook)\n",
        "**Chunking → Hybrid Search → Re-ranking → Grounded QA → Evaluation**\n",
        "\n",
        "**Submission:** Survey  \n",
        "**Submission Date:** January 29 (Thursday), at the end of class  \n",
        "\n",
        "## New Requirement (Important)\n",
        "For **full credit**, you must add **your own explanations** for key steps:\n",
        "\n",
        "- After each **IMPORTANT** code cell, write a short **Cell Description** (2–5 sentences) in a Markdown cell:\n",
        "  - What the cell does\n",
        "  - Why the step matters in a RAG system\n",
        "  - Any assumptions/choices you made (e.g., chunk size, α, embedding model)\n",
        "\n",
        "> Tip: Treat your descriptions like “mini system documentation.” This is how engineers communicate system design.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bada1d7d",
      "metadata": {
        "id": "bada1d7d"
      },
      "source": [
        "## Project Dataset Guide (Required for Full Credit)\n",
        "\n",
        "To earn **full credit (2% individual)** you must run this lab on **your own project-aligned dataset**, not only the benchmark.\n",
        "\n",
        "### Minimum project dataset requirements\n",
        "- **3–20 documents** (start small; you can scale later)\n",
        "- Prefer **plain text** documents (`.txt`) for Lab 2\n",
        "- Total size: **at least ~3–10 pages** of content across all files\n",
        "\n",
        "### Recommended dataset types (choose one)\n",
        "- Course / technical docs (manuals, API docs, tutorials)\n",
        "- Research papers (your topic area) converted to text\n",
        "- Policies / guidelines / compliance docs\n",
        "- Meeting notes / project reports\n",
        "- Domain corpus (healthcare, cybersecurity, business, etc.)\n",
        "\n",
        "### Folder structure (required)\n",
        "Create a folder named `project_data/` and put files inside:\n",
        "- `project_data/doc1.txt`\n",
        "- `project_data/doc2.txt`\n",
        "- ...\n",
        "\n",
        "> If you have PDFs, convert them to text first (instructions below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9ad83fce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ad83fce",
        "outputId": "c3e8c0bc-1b0d-4184-8242-81842a6ffd91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Folder ready: project_data\n",
            "Put 3–20 .txt files into ./project_data/\n",
            "Currently found: 0 txt files\n"
          ]
        }
      ],
      "source": [
        "# ✅ IMPORTANT: Create a project_data folder and add your files\n",
        "import os, glob\n",
        "\n",
        "PROJECT_FOLDER = \"project_data\"\n",
        "os.makedirs(PROJECT_FOLDER, exist_ok=True)\n",
        "\n",
        "print(\"✅ Folder ready:\", PROJECT_FOLDER)\n",
        "print(\"Put 3–20 .txt files into ./project_data/\")\n",
        "print(\"Currently found:\", len(glob.glob(os.path.join(PROJECT_FOLDER, \"*.txt\"))), \"txt files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afa72ebc",
      "metadata": {
        "id": "afa72ebc"
      },
      "source": [
        "### If you are using Google Colab (Upload files)\n",
        "\n",
        "**Option A — Upload manually**\n",
        "1. Click the **Files** icon (left sidebar)\n",
        "2. Click **Upload**\n",
        "3. Upload your `.txt` files\n",
        "4. Move them into `project_data/` (or upload directly into that folder)\n",
        "\n",
        "**Option B — Pull from GitHub**\n",
        "If your project docs are in a GitHub repo, you can clone it and copy files into `project_data/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fb695525",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb695525",
        "outputId": "ea8efbbe-63b7-4e81-a0aa-25887483af8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moved 0 files into project_data/\n",
            "Now found: 0 txt files\n"
          ]
        }
      ],
      "source": [
        "# (Colab only) Optional helper: move uploaded .txt files into project_data/\n",
        "# Skip if you're not in Colab or you already placed files correctly.\n",
        "\n",
        "import shutil, glob, os\n",
        "\n",
        "PROJECT_FOLDER = \"project_data\"\n",
        "os.makedirs(PROJECT_FOLDER, exist_ok=True)\n",
        "\n",
        "moved = 0\n",
        "for fp in glob.glob(\"*.txt\"):\n",
        "    shutil.move(fp, os.path.join(PROJECT_FOLDER, os.path.basename(fp)))\n",
        "    moved += 1\n",
        "\n",
        "print(f\"Moved {moved} files into {PROJECT_FOLDER}/\")\n",
        "print(\"Now found:\", len(glob.glob(os.path.join(PROJECT_FOLDER, '*.txt'))), \"txt files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb5fd603",
      "metadata": {
        "id": "eb5fd603"
      },
      "source": [
        "### If your sources are PDFs (Optional)\n",
        "\n",
        "For Lab 2, we recommend converting PDFs to `.txt` first.\n",
        "\n",
        "**Simple approach (good enough for class):**\n",
        "- Copy/paste text from the PDF into a `.txt` file.\n",
        "\n",
        "**Programmatic approach (optional):**\n",
        "If your PDF is text-based (not scanned), you can extract text using `pypdf`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "726a26fe",
      "metadata": {
        "id": "726a26fe"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: PDF → TXT conversion (only for text-based PDFs)\n",
        "# If your PDFs are scanned images, this won't work well without OCR.\n",
        "\n",
        "# !pip -q install pypdf\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "def pdf_to_txt(pdf_path: str, out_folder: str = \"project_data\"):\n",
        "    from pypdf import PdfReader\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = []\n",
        "    for page in reader.pages:\n",
        "        text.append(page.extract_text() or \"\")\n",
        "    txt = \"\\n\\n\".join(text).strip()\n",
        "\n",
        "    os.makedirs(out_folder, exist_ok=True)\n",
        "    out_path = Path(out_folder) / (Path(pdf_path).stem + \".txt\")\n",
        "    out_path.write_text(txt, encoding=\"utf-8\", errors=\"ignore\")\n",
        "    return str(out_path), len(txt)\n",
        "\n",
        "# Example usage:\n",
        "# out_path, n_chars = pdf_to_txt(\"/content/your_file.pdf\")\n",
        "# print(\"Saved:\", out_path, \"| chars:\", n_chars)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e405c5a8",
      "metadata": {
        "id": "e405c5a8"
      },
      "source": [
        "### Project Queries + Mini Rubric (Required)\n",
        "\n",
        "You must define **3 project queries**:\n",
        "- Q1, Q2: normal (typical user questions)\n",
        "- Q3: ambiguous / tricky (edge case)\n",
        "\n",
        "Also define a **mini rubric** for each query:\n",
        "- What counts as “relevant evidence”? (keywords, entities, definitions, constraints)\n",
        "- What would a correct answer look like? (1–2 bullet points)\n",
        "\n",
        "This rubric makes your evaluation meaningful (Precision@K / Recall@K).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c0a7b3af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0a7b3af",
        "outputId": "a3417e3d-e610-4869-f875-813e10b71440"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Q1': {'query': 'Replace with your project query #1',\n",
              "  'rubric_relevant_evidence': ['List 2–4 evidence requirements (entities, constraints, definitions)'],\n",
              "  'rubric_correct_answer': ['List 1–2 bullets describing what a correct answer should include']},\n",
              " 'Q2': {'query': 'Replace with your project query #2',\n",
              "  'rubric_relevant_evidence': ['…'],\n",
              "  'rubric_correct_answer': ['…']},\n",
              " 'Q3_ambiguous': {'query': 'Replace with your project ambiguous query #3',\n",
              "  'rubric_relevant_evidence': ['…'],\n",
              "  'rubric_correct_answer': ['…']}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# ✅ REQUIRED: Define your project queries and mini rubric\n",
        "project_queries = {\n",
        "    \"Q1\": {\n",
        "        \"query\": \"Replace with your project query #1\",\n",
        "        \"rubric_relevant_evidence\": [\n",
        "            \"List 2–4 evidence requirements (entities, constraints, definitions)\",\n",
        "        ],\n",
        "        \"rubric_correct_answer\": [\n",
        "            \"List 1–2 bullets describing what a correct answer should include\",\n",
        "        ],\n",
        "    },\n",
        "    \"Q2\": {\n",
        "        \"query\": \"Replace with your project query #2\",\n",
        "        \"rubric_relevant_evidence\": [\n",
        "            \"…\",\n",
        "        ],\n",
        "        \"rubric_correct_answer\": [\n",
        "            \"…\",\n",
        "        ],\n",
        "    },\n",
        "    \"Q3_ambiguous\": {\n",
        "        \"query\": \"Replace with your project ambiguous query #3\",\n",
        "        \"rubric_relevant_evidence\": [\n",
        "            \"…\",\n",
        "        ],\n",
        "        \"rubric_correct_answer\": [\n",
        "            \"…\",\n",
        "        ],\n",
        "    },\n",
        "}\n",
        "\n",
        "project_queries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65354e4f",
      "metadata": {
        "id": "65354e4f"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain what files you used for your project dataset, why they match your scenario, and how you designed your 3 queries + rubric.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64580d56",
      "metadata": {
        "id": "64580d56"
      },
      "source": [
        "## 0) One-Click Setup + Import Check  ✅ **IMPORTANT: Add Cell Description after running**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "258416d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "258416d1",
        "outputId": "a53693d6-2fe6-414b-ee41-bc8d36365576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "# CS 5542 Lab 2 — One-Click Dependency Install\n",
        "# If your imports fail after installing, restart the runtime/kernel and rerun this cell.\n",
        "\n",
        "!pip install -q sentence-transformers faiss-cpu chromadb datasets transformers scikit-learn rank-bm25\n",
        "\n",
        "import os, glob, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Dict, Set\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"✅ Setup complete. If you see dependency warnings, ignore unless imports fail.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7441dee8",
      "metadata": {
        "id": "7441dee8"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Write 2–5 sentences explaining what the setup cell does and why restarting the kernel sometimes matters after pip installs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef10f54",
      "metadata": {
        "id": "9ef10f54"
      },
      "source": [
        "## 1) Load Data (Benchmark + Project Data)  ✅ **IMPORTANT: Add Cell Description after running**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9472aec6",
      "metadata": {
        "id": "9472aec6"
      },
      "outputs": [],
      "source": [
        "# Benchmark Loader (classroom-safe fallback; avoids script-based datasets)\n",
        "def load_benchmark(n: int = 120) -> List[str]:\n",
        "    # 1) Try a script-free SciFact source\n",
        "    try:\n",
        "        print(\"Trying allenai/scifact...\")\n",
        "        ds = load_dataset(\"allenai/scifact\", split=f\"train[:{n}]\")\n",
        "        sample = ds[0]\n",
        "        if \"claim\" in sample:\n",
        "            return [x[\"claim\"] for x in ds]\n",
        "        if \"text\" in sample:\n",
        "            return [x[\"text\"] for x in ds]\n",
        "        raise RuntimeError(\"Unknown SciFact schema.\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ allenai/scifact failed:\", str(e))\n",
        "\n",
        "    # 2) Try multi_news\n",
        "    try:\n",
        "        print(\"Trying multi_news...\")\n",
        "        ds = load_dataset(\"multi_news\", split=f\"train[:{n}]\")\n",
        "        return [x[\"document\"] for x in ds]\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ multi_news failed:\", str(e))\n",
        "\n",
        "    # 3) Fallback: ag_news (very stable)\n",
        "    print(\"Using ag_news fallback...\")\n",
        "    ds = load_dataset(\"ag_news\", split=f\"train[:{n}]\")\n",
        "    return [x[\"text\"] for x in ds]\n",
        "\n",
        "# Load benchmark docs\n",
        "benchmark_docs = load_benchmark(n=120)\n",
        "print(f\"Loaded benchmark docs: {len(benchmark_docs)}\")\n",
        "\n",
        "# Load project-aligned docs from ./project_data/*.txt\n",
        "PROJECT_FOLDER = \"project_data\"\n",
        "project_files = sorted(glob.glob(os.path.join(PROJECT_FOLDER, \"*.txt\")))\n",
        "project_docs = []\n",
        "for fp in project_files:\n",
        "    with open(fp, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        project_docs.append(f.read())\n",
        "\n",
        "print(f\"Loaded project docs: {len(project_docs)}\")\n",
        "if len(project_docs) == 0:\n",
        "    print(\"⚠️ Add 3–20 .txt files under ./project_data/ to earn full credit.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c279007",
      "metadata": {
        "id": "5c279007"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain what dataset(s) you loaded and why we require **project-aligned** data for full credit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c89e45e4",
      "metadata": {
        "id": "c89e45e4"
      },
      "source": [
        "## 2) Chunking (Fixed vs Semantic)  ✅ **IMPORTANT: Add Cell Description after running**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b326398",
      "metadata": {
        "id": "0b326398"
      },
      "outputs": [],
      "source": [
        "# --- Chunking functions ---\n",
        "def fixed_chunks(text: str, size: int = 1200, overlap: int = 200) -> List[str]:\n",
        "    \"\"\"Character-based fixed window chunking (fast and reliable in class).\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks = []\n",
        "    step = max(1, size - overlap)\n",
        "    for i in range(0, len(text), step):\n",
        "        c = text[i:i+size].strip()\n",
        "        if len(c) > 50:\n",
        "            chunks.append(c)\n",
        "    return chunks\n",
        "\n",
        "def semantic_chunks(text: str) -> List[str]:\n",
        "    \"\"\"Paragraph-based semantic chunking; merges short segments to keep context.\"\"\"\n",
        "    paras = [p.strip() for p in re.split(r\"\\n\\s*\\n+\", text) if p.strip()]\n",
        "    merged, buf = [], \"\"\n",
        "    for p in paras:\n",
        "        if len(buf) < 400:\n",
        "            buf = (buf + \"\\n\\n\" + p).strip()\n",
        "        else:\n",
        "            merged.append(buf); buf = p\n",
        "    if buf:\n",
        "        merged.append(buf)\n",
        "    return [m for m in merged if len(m) > 80]\n",
        "\n",
        "def build_corpus(docs: List[str], mode: str) -> List[str]:\n",
        "    all_chunks = []\n",
        "    for d in docs:\n",
        "        if mode == \"fixed\":\n",
        "            all_chunks.extend(fixed_chunks(d))\n",
        "        elif mode == \"semantic\":\n",
        "            all_chunks.extend(semantic_chunks(d))\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'fixed' or 'semantic'\")\n",
        "    return all_chunks\n",
        "\n",
        "# Build both corpora and choose one to use in retrieval\n",
        "all_docs = benchmark_docs + project_docs\n",
        "fixed_corpus = build_corpus(all_docs, mode=\"fixed\")\n",
        "semantic_corpus = build_corpus(all_docs, mode=\"semantic\")\n",
        "\n",
        "print(\"Fixed corpus chunks:\", len(fixed_corpus))\n",
        "print(\"Semantic corpus chunks:\", len(semantic_corpus))\n",
        "\n",
        "# Choose the corpus for the lab (recommend semantic for better context)\n",
        "CORPUS = semantic_corpus\n",
        "print(\"✅ Using CORPUS =\", \"semantic\" if CORPUS is semantic_corpus else \"fixed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50bdc0fb",
      "metadata": {
        "id": "50bdc0fb"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain the difference between **fixed** and **semantic** chunking and why chunking affects retrieval quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993631c9",
      "metadata": {
        "id": "993631c9"
      },
      "source": [
        "## 3) Build Retrieval Indexes (Keyword + Vector)  ✅ **IMPORTANT: Add Cell Description after running**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d268ff",
      "metadata": {
        "id": "a1d268ff"
      },
      "outputs": [],
      "source": [
        "# --- Keyword Retrieval (TF-IDF + BM25) ---\n",
        "def tokenize(s: str) -> List[str]:\n",
        "    return re.findall(r\"[A-Za-z0-9]+\", s.lower())\n",
        "\n",
        "tfidf = TfidfVectorizer(stop_words=\"english\", max_features=50000)\n",
        "tfidf_matrix = tfidf.fit_transform(CORPUS)\n",
        "\n",
        "def keyword_tfidf(query: str, k: int = 10) -> List[Tuple[int, float]]:\n",
        "    q_vec = tfidf.transform([query])\n",
        "    scores = (tfidf_matrix @ q_vec.T).toarray().squeeze()\n",
        "    top = np.argsort(scores)[-k:][::-1]\n",
        "    return [(int(i), float(scores[i])) for i in top]\n",
        "\n",
        "bm25 = BM25Okapi([tokenize(x) for x in CORPUS])\n",
        "\n",
        "def keyword_bm25(query: str, k: int = 10) -> List[Tuple[int, float]]:\n",
        "    scores = bm25.get_scores(tokenize(query))\n",
        "    top = np.argsort(scores)[-k:][::-1]\n",
        "    return [(int(i), float(scores[i])) for i in top]\n",
        "\n",
        "# --- Vector Retrieval (SentenceTransformer + FAISS) ---\n",
        "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedder = SentenceTransformer(embed_model_name)\n",
        "\n",
        "embeddings = embedder.encode(CORPUS, show_progress_bar=True, convert_to_numpy=True, normalize_embeddings=True)\n",
        "dim = embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatIP(dim)  # cosine via normalized vectors + inner product\n",
        "faiss_index.add(embeddings)\n",
        "\n",
        "def vector_search(query: str, k: int = 10) -> List[Tuple[int, float]]:\n",
        "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    scores, idx = faiss_index.search(q, k)\n",
        "    return [(int(i), float(s)) for i, s in zip(idx[0], scores[0])]\n",
        "\n",
        "print(\"✅ Retrieval engines ready: TF-IDF, BM25, Vector(FAISS)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af1fdbf8",
      "metadata": {
        "id": "af1fdbf8"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain why we build **both** keyword and vector retrieval engines, and when each one is expected to work best.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5c8b2a",
      "metadata": {
        "id": "9b5c8b2a"
      },
      "source": [
        "## 4) Hybrid Retrieval (α-Weighted Fusion)  ✅ **IMPORTANT: Add Cell Description after running**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d9071c6",
      "metadata": {
        "id": "4d9071c6"
      },
      "outputs": [],
      "source": [
        "def normalize_scores(pairs: List[Tuple[int, float]]) -> Dict[int, float]:\n",
        "    if not pairs:\n",
        "        return {}\n",
        "    vals = np.array([s for _, s in pairs], dtype=float)\n",
        "    vmin, vmax = vals.min(), vals.max()\n",
        "    if vmax - vmin < 1e-9:\n",
        "        return {i: 1.0 for i, _ in pairs}\n",
        "    return {i: (s - vmin) / (vmax - vmin) for i, s in pairs}\n",
        "\n",
        "def hybrid_search(query: str, k_keyword: int = 10, k_vector: int = 10, alpha: float = 0.5,\n",
        "                  top_k: int = 10, keyword_mode: str = \"bm25\") -> List[Tuple[int, float]]:\n",
        "    kw = keyword_bm25(query, k=k_keyword) if keyword_mode == \"bm25\" else keyword_tfidf(query, k=k_keyword)\n",
        "    vec = vector_search(query, k=k_vector)\n",
        "\n",
        "    kw_n = normalize_scores(kw)\n",
        "    vec_n = normalize_scores(vec)\n",
        "\n",
        "    all_ids = set(kw_n) | set(vec_n)\n",
        "    combined = []\n",
        "    for i in all_ids:\n",
        "        score = alpha * kw_n.get(i, 0.0) + (1 - alpha) * vec_n.get(i, 0.0)\n",
        "        combined.append((i, float(score)))\n",
        "\n",
        "    combined.sort(key=lambda x: x[1], reverse=True)\n",
        "    return combined[:top_k]\n",
        "\n",
        "print(\"✅ Hybrid retrieval ready. You'll sweep alpha ∈ {0.2, 0.5, 0.8}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "132cee1f",
      "metadata": {
        "id": "132cee1f"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain what **hybrid fusion** is and what the α parameter means (semantic-heavy vs keyword-heavy).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "797b9061",
      "metadata": {
        "id": "797b9061"
      },
      "source": [
        "## 5) Re-ranking (Cross-Encoder if available)  ✅ **IMPORTANT: Add Cell Description after running**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84aa1a0d",
      "metadata": {
        "id": "84aa1a0d"
      },
      "outputs": [],
      "source": [
        "USE_CROSS_ENCODER = True\n",
        "reranker = None\n",
        "\n",
        "if USE_CROSS_ENCODER:\n",
        "    try:\n",
        "        from sentence_transformers import CrossEncoder\n",
        "        reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "        print(\"✅ Cross-encoder reranker loaded.\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠️ Cross-encoder not available. Falling back to no reranking.\")\n",
        "        print(\"Error:\", e)\n",
        "        reranker = None\n",
        "\n",
        "def rerank(query: str, candidates: List[Tuple[int, float]], top_k: int = 5) -> List[Tuple[int, float]]:\n",
        "    ids = [i for i, _ in candidates]\n",
        "    if reranker is None:\n",
        "        return candidates[:top_k]\n",
        "    pairs = [(query, CORPUS[i]) for i in ids]\n",
        "    scores = reranker.predict(pairs)\n",
        "    scored = list(zip(ids, scores))\n",
        "    scored.sort(key=lambda x: x[1], reverse=True)\n",
        "    return [(int(i), float(s)) for i, s in scored[:top_k]]\n",
        "\n",
        "print(\"✅ Reranking function ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fdc27d6",
      "metadata": {
        "id": "4fdc27d6"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain what reranking does and why it often improves Precision@K (but costs extra compute).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54738cf3",
      "metadata": {
        "id": "54738cf3"
      },
      "source": [
        "## 6) Run Your 3 Project Queries + Generate Answers  ✅ **IMPORTANT: Add Cell Description after running**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05dfbd8b",
      "metadata": {
        "id": "05dfbd8b"
      },
      "outputs": [],
      "source": [
        "# Generator (small + class-friendly)\n",
        "gen = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "\n",
        "def prompt_only_answer(query: str, max_new_tokens: int = 200) -> str:\n",
        "    return gen(query, max_new_tokens=max_new_tokens)[0][\"generated_text\"]\n",
        "\n",
        "def rag_answer(query: str, chunk_ids: List[int], max_new_tokens: int = 220) -> str:\n",
        "    evidence = \"\\n\\n\".join([f\"[Chunk {j+1}] {CORPUS[i]}\" for j, i in enumerate(chunk_ids)])\n",
        "    prompt = f\"\"\"Answer the question using ONLY the evidence below.\n",
        "\n",
        "Evidence:\n",
        "{evidence}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Rules:\n",
        "- If evidence is insufficient, say: Not enough evidence.\n",
        "- Cite evidence with [Chunk 1], [Chunk 2], etc.\n",
        "\"\"\"\n",
        "    return gen(prompt, max_new_tokens=max_new_tokens)[0][\"generated_text\"]\n",
        "\n",
        "def show_top(pairs: List[Tuple[int, float]], title: str, k: int = 5):\n",
        "    print(f\"\\n=== {title} (Top {k}) ===\")\n",
        "    for r, (i, s) in enumerate(pairs[:k], 1):\n",
        "        snip = CORPUS[i].replace(\"\\n\", \" \")\n",
        "        snip = snip[:220] + (\"...\" if len(snip) > 220 else \"\")\n",
        "        print(f\"{r:>2}. id={i:<6} score={s:>8.4f} | {snip}\")\n",
        "\n",
        "# ✅ REQUIRED: Replace with your project queries\n",
        "queries = [\n",
        "    \"Q1: \" + project_queries[\"Q1\"][\"query\"],\n",
        "    \"Q2: \" + project_queries[\"Q2\"][\"query\"],\n",
        "    \"Q3 (ambiguous): \" + project_queries[\"Q3_ambiguous\"][\"query\"],\n",
        "]\n",
        "\n",
        "alphas = [0.2, 0.5, 0.8]\n",
        "results_summary = []\n",
        "\n",
        "for q in queries:\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(q)\n",
        "\n",
        "    kw = keyword_bm25(q, k=10)\n",
        "    vec = vector_search(q, k=10)\n",
        "    show_top(kw, \"BM25 Keyword\")\n",
        "    show_top(vec, \"Vector (FAISS)\")\n",
        "\n",
        "    hybrids = []\n",
        "    for a in alphas:\n",
        "        hyb = hybrid_search(q, alpha=a, top_k=10, keyword_mode=\"bm25\")\n",
        "        hybrids.append((a, hyb))\n",
        "        show_top(hyb, f\"Hybrid (alpha={a})\")\n",
        "\n",
        "    best_a, _ = max(hybrids, key=lambda t: np.mean([s for _, s in t[1]]) if t[1] else -1)\n",
        "    print(f\"\\nSelected hybrid alpha={best_a}\")\n",
        "\n",
        "    candidate_pool = hybrid_search(q, alpha=best_a, top_k=20, keyword_mode=\"bm25\")\n",
        "    reranked = rerank(q, candidate_pool, top_k=5)\n",
        "    show_top(reranked, \"Re-ranked\")\n",
        "\n",
        "    top3_ids = [i for i, _ in reranked[:3]]\n",
        "    print(\"\\nTop-3 evidence chunk IDs:\", top3_ids)\n",
        "\n",
        "    po = prompt_only_answer(q)\n",
        "    ra = rag_answer(q, top3_ids)\n",
        "\n",
        "    print(\"\\n--- Prompt-only answer ---\\n\", po)\n",
        "    print(\"\\n--- RAG-grounded answer ---\\n\", ra)\n",
        "\n",
        "    results_summary.append({\n",
        "        \"query\": q,\n",
        "        \"best_alpha\": best_a,\n",
        "        \"top3_chunk_ids\": top3_ids,\n",
        "        \"prompt_only\": po,\n",
        "        \"rag\": ra,\n",
        "    })\n",
        "\n",
        "results_summary[:1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b7f6c61",
      "metadata": {
        "id": "2b7f6c61"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain how you compared keyword/vector/hybrid retrieval, how you selected α, and how reranking affected the evidence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48fa878e",
      "metadata": {
        "id": "48fa878e"
      },
      "source": [
        "## 7) Metrics (Precision@5 / Recall@10) + Manual Relevance Labels  ✅ **IMPORTANT: Add Cell Description after running**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c90307",
      "metadata": {
        "id": "03c90307"
      },
      "outputs": [],
      "source": [
        "def precision_at_k(retrieved: List[int], relevant: Set[int], k: int = 5) -> float:\n",
        "    top = retrieved[:k]\n",
        "    if not top:\n",
        "        return 0.0\n",
        "    return sum(1 for i in top if i in relevant) / len(top)\n",
        "\n",
        "def recall_at_k(retrieved: List[int], relevant: Set[int], k: int = 10) -> float:\n",
        "    if not relevant:\n",
        "        return 0.0\n",
        "    return len(set(retrieved[:k]) & relevant) / len(relevant)\n",
        "\n",
        "# ✅ REQUIRED: Label a small set of relevant chunk IDs for each query (after inspecting retrieval results).\n",
        "relevance_labels = {q: set() for q in queries}\n",
        "relevance_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bad5400",
      "metadata": {
        "id": "5bad5400"
      },
      "source": [
        "### ✍️ Cell Description (Student)\n",
        "Explain what Precision@K and Recall@K mean in the context of RAG retrieval, and how you labeled relevance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5cc03b9",
      "metadata": {
        "id": "d5cc03b9"
      },
      "outputs": [],
      "source": [
        "def evaluate_query(q: str, relevant: Set[int], alpha: float):\n",
        "    kw_ids = [i for i, _ in keyword_bm25(q, k=10)]\n",
        "    vec_ids = [i for i, _ in vector_search(q, k=10)]\n",
        "    hyb_ids = [i for i, _ in hybrid_search(q, alpha=alpha, top_k=10, keyword_mode=\"bm25\")]\n",
        "    return {\n",
        "        \"P@5_keyword\": precision_at_k(kw_ids, relevant, k=5),\n",
        "        \"R@10_keyword\": recall_at_k(kw_ids, relevant, k=10),\n",
        "        \"P@5_vector\": precision_at_k(vec_ids, relevant, k=5),\n",
        "        \"R@10_vector\": recall_at_k(vec_ids, relevant, k=10),\n",
        "        \"P@5_hybrid\": precision_at_k(hyb_ids, relevant, k=5),\n",
        "        \"R@10_hybrid\": recall_at_k(hyb_ids, relevant, k=10),\n",
        "    }\n",
        "\n",
        "metrics_rows = []\n",
        "for row in results_summary:\n",
        "    q = row[\"query\"]\n",
        "    alpha = row[\"best_alpha\"]\n",
        "    rel = relevance_labels.get(q, set())\n",
        "    m = evaluate_query(q, rel, alpha)\n",
        "    m.update({\"query\": q, \"alpha_used\": alpha, \"num_relevant_labeled\": len(rel)})\n",
        "    metrics_rows.append(m)\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_rows)\n",
        "metrics_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f32aad",
      "metadata": {
        "id": "f5f32aad"
      },
      "source": [
        "## 8) README Checklist (Deliverables)\n",
        "\n",
        "Create a section titled **Lab 2 — Advanced RAG Results** in your repo README and include:\n",
        "- Results table (Query × Method × Precision@5 / Recall@10)\n",
        "- Screenshots: chunking comparison, reranking before/after, prompt-only vs RAG answers\n",
        "- Reflection (3–5 sentences): one failure case, which layer failed, one concrete fix\n",
        "\n",
        "### Required Reflection Labels\n",
        "- Chunking failure\n",
        "- Retrieval failure\n",
        "- Re-ranking failure\n",
        "- Generation failure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5378dd8b",
      "metadata": {
        "id": "5378dd8b"
      },
      "source": [
        "## 9) Final Requirement Reminder (2% Individual)\n",
        "To earn full credit, you must demonstrate:\n",
        "- **Project-aligned data** (your domain corpus)\n",
        "- **Three domain queries** (including one ambiguous case)\n",
        "- **One system customization** (chunking choice, α policy, model choice, etc.)\n",
        "- **One real failure case + fix**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
